{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"40d538b3c8a6460b81660b64ed48d35b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0f9a1dc5e65a48f087cfe31be1461610","IPY_MODEL_cf3b799d97834f109d37070cbcfb0972","IPY_MODEL_3fffce12728042eeade95e13e4086f44"],"layout":"IPY_MODEL_afc5b4e033db406eb2ce8e0a691bc469"}},"0f9a1dc5e65a48f087cfe31be1461610":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c83e473170304943bbc06d89b5a523d7","placeholder":"​","style":"IPY_MODEL_fc4fa7a70e4f41e7869358c56725d574","value":"100%"}},"cf3b799d97834f109d37070cbcfb0972":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbeffdb532954b8d84ea887f3adb1133","max":182,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b70665d762624745a97e733f3d18e8c0","value":182}},"3fffce12728042eeade95e13e4086f44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e174c751fe754b2895d40b49bf95bf69","placeholder":"​","style":"IPY_MODEL_0c0382b082e24b7fb99e04ace3c0ca60","value":" 182/182 [00:16&lt;00:00, 15.03it/s]"}},"afc5b4e033db406eb2ce8e0a691bc469":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c83e473170304943bbc06d89b5a523d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc4fa7a70e4f41e7869358c56725d574":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbeffdb532954b8d84ea887f3adb1133":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b70665d762624745a97e733f3d18e8c0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e174c751fe754b2895d40b49bf95bf69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c0382b082e24b7fb99e04ace3c0ca60":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["#@title 必要なライブラリのインストール\n","\n","!pip install -q ultralytics\n","!git clone https://github.com/edihbrandon/RictyDiminished.git\n","\n","import colorsys\n","import os\n","import random\n","import re\n","\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import ImageFont\n","from pydantic import BaseModel\n","from tqdm.notebook import tqdm\n","from ultralytics import YOLO"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AvEwTyYPkJgc","executionInfo":{"status":"ok","timestamp":1736924582038,"user_tz":-540,"elapsed":11908,"user":{"displayName":"川崎雄太","userId":"13708483500900137948"}},"outputId":"c4522bd7-7517-46dd-de3f-e9178ac3e023"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'RictyDiminished' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["#@title Google Driveに接続\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rDHIISdNfatG","executionInfo":{"status":"ok","timestamp":1736924583662,"user_tz":-540,"elapsed":1627,"user":{"displayName":"川崎雄太","userId":"13708483500900137948"}},"outputId":"2770d914-3587-4f50-9bb8-b945871060c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["下記のURLから動画ファイルをダウンロードしてください。サイズは1080×1920にしてください。ダウンロード後、Google Colabにアップロードしてください。\n","\n","\n","\n","**[動画のURL](https://pixabay.com/ja/videos/%E5%AD%A6%E7%94%9F-%E6%95%99%E8%82%B2-%E5%AD%A6%E6%A0%A1-%E8%AA%AD%E3%82%80-%E5%AD%A6%E3%81%B6-215472/)**\n","\n","\n","> <a href=\"https://pixabay.com/ja//?utm_source=link-attribution&utm_medium=referral&utm_campaign=video&utm_content=215472\">Pixabay</a>が提供する<a href=\"https://pixabay.com/ja/users/vacampbe-44247746/?utm_source=link-attribution&utm_medium=referral&utm_campaign=video&utm_content=215472\">Virginia Campbell</a>の動画\n","\n"],"metadata":{"id":"nXsZeS59Ub39"}},{"cell_type":"code","source":["video_dir = \"/content/drive/MyDrive/Pixabay\" # @param {type:\"string\"}\n","video_filename = \"215472_small.mp4\" # @param {type:\"string\"}"],"metadata":{"id":"FLRkrsO4Uyk2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Configオブジェクト（設定値を格納するオブジェクト）\n","\n","class Config(BaseModel):\n","    video_dir: str\n","    video_filename: str\n","\n","    @property\n","    def video_src_path(self):\n","        return os.path.join(self.video_dir, self.video_filename)"],"metadata":{"id":"JU_emd_FjSI7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title BBoxの定義\n","\n","PATH_FONT = \"/content/RictyDiminished/RictyDiminishedDiscord-Regular.ttf\"\n","FONT_CACHE: dict[int, ImageFont.FreeTypeFont] = {}\n","CHAR_CACHE: dict[str, dict[int, tuple[bool, tuple[int, int, int, int], np.ndarray]]] = {}\n","DEFAULT_TEXT_COLOR = (0, 0, 255)\n","DEFAULT_PADDING_TOP = 20\n","TEXT_OFFSET = 5\n","\n","class BBox(BaseModel):\n","    left: float\n","    top: float\n","    right: float\n","    bottom: float\n","    score: float\n","    label: int\n","\n","    @property\n","    def bottom_center(self) -> tuple[float, float]:\n","        return ((self.left + self.right) / 2, self.bottom)\n","\n","    def draw(self, img: np.ndarray, classnames: dict[int, str], color_mapping: dict[str, tuple[int, int, int]], alert: bool, another_text_list: list[tuple[str, tuple[int, int, int]]]):\n","        color = (DEFAULT_TEXT_COLOR if alert else color_mapping.get(classnames[self.label], DEFAULT_TEXT_COLOR))\n","\n","        # Draw bounding box\n","        cv2.rectangle(\n","            img,\n","            pt1=(int(self.left), int(self.top)),\n","            pt2=(int(self.right), int(self.bottom)),\n","            color=color,\n","            thickness=2,\n","        )\n","\n","        # Draw label text\n","        self._put_text(img, classnames.get(self.label, 'not found'), color, 0)\n","\n","        # Draw additional texts\n","        padding_top = DEFAULT_PADDING_TOP\n","        for text, text_color in another_text_list:\n","            padding_top += self._put_text(img, text, text_color, padding_top)\n","\n","    def _put_text(self, img: np.ndarray, text: str, color: tuple[int, int, int], padding_top: int) -> int:\n","        text = re.sub(r\"[\\t\\n\\r]\", \"\", text)\n","        h, w, *_ = img.shape\n","        d = img.ndim\n","        offset = TEXT_OFFSET\n","        max_height = 0\n","\n","        for char in text:\n","            jp, bbox, mask = self._get_char_mask(char, size=20)\n","            li, ti, ri, bi = (\n","                int(self.left) + bbox[0] + offset,\n","                int(self.top) + bbox[1] + padding_top,\n","                int(self.left) + bbox[2] + offset,\n","                int(self.top) + bbox[3] + padding_top,\n","            )\n","\n","            offset += bbox[2] - bbox[0]\n","            max_height = max(max_height, bbox[3] - bbox[1])\n","\n","            # Adjust clipping\n","            lm, tm, rm, bm = 0, 0, ri - li, bi - ti\n","            li, lm = max(0, li), lm - min(0, li)\n","            ti, tm = max(0, ti), tm - min(0, ti)\n","            ri, rm = min(w, ri), rm - max(0, ri - w)\n","            bi, bm = min(h, bi), bm - max(0, bi - h)\n","\n","            if (ri - li) <= 0 or (bi - ti) <= 0:\n","                continue\n","\n","            # Apply mask to image\n","            if d == 3:\n","                img[ti:bi, li:ri] = img[ti:bi, li:ri] * (1 - mask[tm:bm, lm:rm, np.newaxis]) + color * mask[tm:bm, lm:rm, np.newaxis]\n","            else:\n","                img[ti:bi, li:ri] = img[ti:bi, li:ri] * (1 - mask[tm:bm, lm:rm]) + color * mask[tm:bm, lm:rm]\n","\n","        return max_height\n","\n","    @staticmethod\n","    def _get_char_mask(char: str, size: int):\n","        def _get_font(size: int) -> ImageFont.FreeTypeFont:\n","            if size not in FONT_CACHE:\n","                FONT_CACHE[size] = ImageFont.truetype(PATH_FONT, size)\n","            return FONT_CACHE[size]\n","\n","        if char not in CHAR_CACHE:\n","            CHAR_CACHE[char] = {}\n","\n","        if size not in CHAR_CACHE[char]:\n","            font = _get_font(size)\n","            bbox = font.getbbox(char)\n","            mask = np.asarray(font.getmask(char, \"L\"), dtype=np.float32).reshape(bbox[3] - bbox[1], bbox[2] - bbox[0]) / 255\n","            CHAR_CACHE[char][size] = (not char.isascii(), bbox, mask)\n","\n","        return CHAR_CACHE[char][size]"],"metadata":{"id":"eZJaC3DiwHJ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Skeltonの定義\n","\n","_NUM_JOINTS = 17\n","_EDGES = [\n","    [0, 1],\n","    [0, 2],\n","    [1, 3],\n","    [2, 4],\n","    [3, 5],\n","    [4, 6],\n","    [5, 6],\n","    [5, 7],\n","    [7, 9],\n","    [6, 8],\n","    [8, 10],\n","    [5, 11],\n","    [6, 12],\n","    [11, 12],\n","    [11, 13],\n","    [13, 15],\n","    [12, 14],\n","    [14, 16],\n","]\n","_EC = [\n","    (195, 255, 1),\n","    (0, 255, 0),\n","    (195, 255, 1),\n","    (0, 255, 0),\n","    (195, 255, 1),\n","    (0, 255, 0),\n","    (195, 255, 195),\n","    (195, 255, 1),\n","    (195, 255, 1),\n","    (0, 255, 0),\n","    (0, 255, 0),\n","    (195, 255, 1),\n","    (0, 255, 0),\n","    (195, 255, 195),\n","    (195, 255, 1),\n","    (195, 255, 1),\n","    (0, 255, 0),\n","    (0, 255, 0),\n","]\n","_COLORS_HP = [\n","    (195, 255, 195),\n","    (195, 255, 1),\n","    (0, 255, 0),\n","    (195, 255, 1),\n","    (0, 255, 0),\n","    (195, 255, 1),\n","    (0, 255, 0),\n","    (195, 255, 1),\n","    (0, 255, 0),\n","    (195, 255, 1),\n","    (0, 255, 0),\n","    (195, 255, 1),\n","    (0, 255, 0),\n","    (195, 255, 1),\n","    (0, 255, 0),\n","    (195, 255, 1),\n","    (0, 255, 0),\n","]\n","\n","class Skeleton:\n","    def __init__(self, key_points: np.ndarray, score: np.ndarray):\n","        self.key_points: np.ndarray = key_points\n","        self.score: np.ndarray = score\n","\n","    def draw(self, img: np.ndarray):\n","        points = self.key_points\n","\n","        for j in range(_NUM_JOINTS):\n","            cv2.circle(img, (points[j, 0], points[j, 1]), 5, _COLORS_HP[j], -1)\n","\n","        for j, e in enumerate(_EDGES):\n","            if points[e].min() <= 0:\n","                continue\n","            cv2.line(\n","                img,\n","                (points[e[0], 0], points[e[0], 1]),\n","                (points[e[1], 0], points[e[1], 1]),\n","                _EC[j],\n","                2,\n","                lineType=cv2.LINE_AA,\n","            )"],"metadata":{"id":"vjJ27kkSwPuR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Detectionオブジェクト（結果を格納するオブジェクト）\n","\n","class Detection:\n","    def __init__(self, bbox: BBox, skeleton: Skeleton | None):\n","        self.bbox: BBox = bbox\n","        self.skeleton: Skeleton | None = skeleton\n","        self.alert = False\n","        self.another_text_list = []\n","\n","\n","    def draw(self, img, classnames, color_mapping):\n","        \"\"\"Bounding boxとSkeletonを描画するメソッド\"\"\"\n","        self.bbox.draw(img, classnames, color_mapping, self.alert, self.another_text_list)\n","\n","        if isinstance(self.skeleton, Skeleton):\n","            self.skeleton.draw(img)\n","\n","    @classmethod\n","    def postprocess(cls, result) -> \"Detection\":\n","        \"\"\"推論結果を受け取り、Detectionインスタンスを生成する\"\"\"\n","        bbox = cls._create_bbox(result)\n","        skeleton = cls._create_skeleton(result) if result.keypoints is not None else None\n","        return cls(bbox, skeleton)\n","\n","    @staticmethod\n","    def _create_bbox(result) -> BBox:\n","        \"\"\"Bboxオブジェクトを生成\"\"\"\n","        xyxy = result.boxes.xyxy.cpu().numpy()[0]\n","        return BBox(\n","            left=int(xyxy[0]),\n","            top=int(xyxy[1]),\n","            right=int(xyxy[2]),\n","            bottom=int(xyxy[3]),\n","            score=float(result.boxes.conf.cpu().numpy()[0]),\n","            label=int(result.boxes.cls.cpu().numpy()[0])\n","        )\n","\n","    @staticmethod\n","    def _create_skeleton(result) -> Skeleton:\n","        \"\"\"Skeletonオブジェクトを生成\"\"\"\n","        keypoints = result.keypoints.data.cpu().numpy()[0]\n","        return Skeleton(\n","            key_points=keypoints[:, :2].astype(int),\n","            score=keypoints[:, 2]\n","        )"],"metadata":{"id":"dNckpzI-khnY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Driftingオブジェクト（フレームごとの結果を保存するオブジェクト）\n","\n","class Drifting:\n","    def __init__(self, img, count, detections = []):\n","        self.img: np.ndarray = img\n","        self.result_img: np.ndarray = img.copy()\n","        self.count: int = count\n","        self.detections: list[Detection] = detections"],"metadata":{"id":"05ERroj0jh4Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title 動画読み込みエレメント\n","\n","class VideoSrc:\n","    def __init__(self, config: Config):\n","        self.frame_num = 0\n","        self.video_src_path = config.video_src_path\n","        self.cap = cv2.VideoCapture(self.video_src_path)\n","        self.frame_count = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","        self.fps = round(self.cap.get(cv2.CAP_PROP_FPS))\n","        self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","        self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","\n","    def read(self) -> Drifting | None:\n","        _, img = self.cap.read()\n","\n","        if _ is False:\n","            return None\n","\n","        drifting = Drifting(\n","            img,\n","            self.frame_num,\n","            []\n","        )\n","        print(f\"\\rcount: {self.frame_num} / {self.frame_count}\", end=\"\")\n","        self.frame_num += 1\n","        return drifting\n","\n","    def reload(self):\n","        self.cap = cv2.VideoCapture(self.video_src_path)\n","        self.frame_num = 0\n","\n","    def release(self):\n","        self.frame_num = 0\n","        self.cap.release()"],"metadata":{"id":"6N1CzWHTjjX9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title 動画書き込みエレメント\n","\n","class VideoSink:\n","    def __init__(self, src: VideoSrc):\n","        self.output_path = self.generate_output_path(src.video_src_path)\n","        fmt = cv2.VideoWriter_fourcc(\"m\",\"p\",\"4\",\"v\")\n","        frame_rate = src.fps\n","        size = (src.width, src.height)\n","        self.writer = cv2.VideoWriter(self.output_path, fmt, frame_rate, size)\n","\n","    def write(self, drifting: Drifting) -> Drifting:\n","        self.writer.write(drifting.result_img)\n","        return drifting\n","\n","    def release(self):\n","        self.writer.release()\n","\n","    @staticmethod\n","    def generate_output_path(file_path: str) -> str:\n","        file_name_without_ext, _ = os.path.splitext(file_path)\n","        new_file_path = f\"{file_name_without_ext}_out.mp4\"\n","        return new_file_path"],"metadata":{"id":"w2_C2XvAknPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title 姿勢推定エレメント\n","\n","class KeyPointDetector:\n","    def __init__(self, conf=0.25):\n","        self.model = YOLO('yolov8n-pose.pt')\n","        self.classnames = self.model.names\n","        self.confidence_threshold = conf\n","\n","\n","    def predict(self, drifting: Drifting) -> Drifting:\n","        results = self.model.predict(drifting.img, conf=self.confidence_threshold)\n","        drifting.detections += [Detection.postprocess(result) for result in results[0]]\n","        return drifting"],"metadata":{"id":"-6x1vuKAkoqD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title 結果の描画エレメント\n","\n","class DetectionRenderer:\n","    def __init__(self, classnames):\n","        self.GOLDEN_RATIO = 0.618033988749895\n","        self.classnames = classnames\n","        self.color_mapping = {\n","            element: (0, 255, 0) if element == \"person\" else self.get_color(idx)\n","            for idx, element in enumerate(self.classnames.values())\n","        }\n","\n","    def draw(self, drifting: Drifting) -> Drifting:\n","        for det in drifting.detections:\n","            det.draw(drifting.result_img, self.classnames, self.color_mapping)\n","\n","        return drifting\n","\n","    def get_color(self, idx: int, s: float = 0.8, vmin: float = 0.7) -> tuple[int, int, int]:\n","        h = np.fmod(idx * self.GOLDEN_RATIO, 1.0)\n","        v = 1.0 - np.fmod(idx * self.GOLDEN_RATIO, 1.0 - vmin)\n","        r, g, b = colorsys.hsv_to_rgb(h, s, v)\n","        return (int(255 * b), int(255 * g), int(255 * r))"],"metadata":{"id":"2tbnnhFS1sva"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title 挙手判定エレメント\n","\n","class RaiseHandWatcher:\n","   \"\"\"\n","   人物の挙手動作を検出するクラス\n","   骨格情報から手首と肩の位置関係を分析して挙手を判定する\n","   \"\"\"\n","   def __init__(self):\n","       \"\"\"初期化メソッド\"\"\"\n","       pass\n","\n","   def detect(self, drifting):\n","       \"\"\"\n","       検出された人物が挙手しているかを判定するメソッド\n","\n","       Args:\n","           drifting: 検出結果と骨格情報を含むDriftingオブジェクト\n","\n","       Returns:\n","           Drifting: アラート情報を追加したDriftingオブジェクト\n","\n","       Note:\n","           挙手の判定基準：\n","           - 左右どちらかの手首が肩の高さより上にある\n","           - 手首の座標が画像内に存在する\n","       \"\"\"\n","       RED = (0, 0, 255)  # BGR形式での赤色\n","\n","       # 各検出人物について処理\n","       for det in drifting.detections:\n","           # 骨格情報がない場合はスキップ\n","           if det.skeleton is None:\n","               continue\n","\n","           # 左右の手首の座標を取得\n","           left_wrist = det.skeleton.key_points[9]   # 左手首\n","           right_wrist = det.skeleton.key_points[10] # 右手首\n","\n","           # 両肩のY座標の平均を計算（肩の高さの基準とする）\n","           shoulder_h = (det.skeleton.key_points[5][1] + det.skeleton.key_points[6][1]) / 2\n","\n","           # 挙手判定\n","           # 1. 両手首が画像内にあること\n","           # 2. どちらかの手首が肩より上にあること\n","           if (self.is_in_image_shape(drifting.img, left_wrist) and\n","               self.is_in_image_shape(drifting.img, right_wrist) and\n","               (left_wrist[1] < shoulder_h or right_wrist[1] < shoulder_h)):\n","               # 挙手していると判定\n","               det.alert = True\n","               det.another_text_list.append(\n","                   (\"挙手しています\", RED)\n","               )\n","           else:\n","               # 挙手していないと判定\n","               det.alert = False\n","\n","       return drifting\n","\n","   @staticmethod\n","   def is_in_image_shape(img: np.ndarray, point: tuple) -> bool:\n","       \"\"\"\n","       指定された点が画像の範囲内にあるかを判定する静的メソッド\n","\n","       Args:\n","           img: 対象の画像（NumPy配列）\n","           point: 判定する点の座標 (x, y)\n","\n","       Returns:\n","           bool: 点が画像範囲内にある場合True、そうでない場合False\n","\n","       Note:\n","           - x座標は0から画像の幅-1の範囲内\n","           - y座標は0から画像の高さ-1の範囲内\n","       \"\"\"\n","       return (0 <= point[0] < img.shape[1]) and (0 <= point[1] < img.shape[0])"],"metadata":{"id":"WHVQcjk-1yFI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 基本設定の初期化\n","config = Config(\n","    video_dir=video_dir,          # 動画ファイルのディレクトリ\n","    video_filename=video_filename  # 動画ファイル名\n",")"],"metadata":{"id":"qzenSNx0w1lT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 動画の入出力を準備\n","video_src = VideoSrc(config)              # 入力動画の読み込み\n","video_sink = VideoSink(video_src)         # 出力動画の設定\n","\n","# 解析モジュールの準備\n","video_src = VideoSrc(config)\n","video_sink = VideoSink(video_src)\n","keypoint_detector = KeyPointDetector(conf=0.5)\n","detection_renderer = DetectionRenderer(keypoint_detector.classnames)\n","raise_hand_watcher = RaiseHandWatcher()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kjR2LzDAw4sM","executionInfo":{"status":"ok","timestamp":1736924617869,"user_tz":-540,"elapsed":1627,"user":{"displayName":"川崎雄太","userId":"13708483500900137948"}},"outputId":"0431f25c-00eb-4c88-a3de-dc01d8443c2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n-pose.pt to 'yolov8n-pose.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6.52M/6.52M [00:00<00:00, 115MB/s]\n"]}]},{"cell_type":"code","source":["# フレームごとの処理を開始\n","for _ in tqdm(range(video_src.frame_count)):\n","    # 1. フレームの読み込み\n","    drifting = video_src.read()\n","    if drifting is None:  # 動画終了のチェック\n","        break\n","\n","    # 2. 姿勢推定の実行\n","    drifting = keypoint_detector.predict(drifting)\n","\n","    # 3. 挙手判定\n","    drifting = raise_hand_watcher.detect(drifting)\n","\n","    # 4. 検出結果の描画\n","    drifting = detection_renderer.draw(drifting)\n","\n","    # 5. 結果の書き出し\n","    drifting = video_sink.write(drifting)\n","\n","# 6. 終了処理\n","video_sink.release()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["40d538b3c8a6460b81660b64ed48d35b","0f9a1dc5e65a48f087cfe31be1461610","cf3b799d97834f109d37070cbcfb0972","3fffce12728042eeade95e13e4086f44","afc5b4e033db406eb2ce8e0a691bc469","c83e473170304943bbc06d89b5a523d7","fc4fa7a70e4f41e7869358c56725d574","fbeffdb532954b8d84ea887f3adb1133","b70665d762624745a97e733f3d18e8c0","e174c751fe754b2895d40b49bf95bf69","0c0382b082e24b7fb99e04ace3c0ca60"]},"id":"CmILLEIxCp0y","executionInfo":{"status":"ok","timestamp":1736924636872,"user_tz":-540,"elapsed":16527,"user":{"displayName":"川崎雄太","userId":"13708483500900137948"}},"outputId":"265ec637-96c6-41a2-91ca-ee4e9b632837"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/182 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40d538b3c8a6460b81660b64ed48d35b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\rcount: 0 / 182\n","0: 640x384 2 persons, 114.6ms\n","Speed: 18.6ms preprocess, 114.6ms inference, 1155.9ms postprocess per image at shape (1, 3, 640, 384)\n","count: 1 / 182\n","0: 640x384 2 persons, 14.8ms\n","Speed: 3.6ms preprocess, 14.8ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 2 / 182\n","0: 640x384 2 persons, 11.6ms\n","Speed: 3.2ms preprocess, 11.6ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n","count: 3 / 182\n","0: 640x384 2 persons, 13.6ms\n","Speed: 4.3ms preprocess, 13.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 4 / 182\n","0: 640x384 2 persons, 11.6ms\n","Speed: 3.2ms preprocess, 11.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 5 / 182\n","0: 640x384 2 persons, 12.1ms\n","Speed: 3.5ms preprocess, 12.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 6 / 182\n","0: 640x384 2 persons, 23.4ms\n","Speed: 3.9ms preprocess, 23.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 7 / 182\n","0: 640x384 2 persons, 14.1ms\n","Speed: 4.3ms preprocess, 14.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n","count: 8 / 182\n","0: 640x384 2 persons, 14.6ms\n","Speed: 4.2ms preprocess, 14.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 384)\n","count: 9 / 182\n","0: 640x384 2 persons, 14.9ms\n","Speed: 3.3ms preprocess, 14.9ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 10 / 182\n","0: 640x384 2 persons, 29.7ms\n","Speed: 3.9ms preprocess, 29.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n","count: 11 / 182\n","0: 640x384 3 persons, 17.0ms\n","Speed: 3.9ms preprocess, 17.0ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 384)\n","count: 12 / 182\n","0: 640x384 3 persons, 13.0ms\n","Speed: 3.6ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 13 / 182\n","0: 640x384 2 persons, 11.1ms\n","Speed: 4.2ms preprocess, 11.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 14 / 182\n","0: 640x384 2 persons, 13.3ms\n","Speed: 3.0ms preprocess, 13.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 15 / 182\n","0: 640x384 3 persons, 14.0ms\n","Speed: 5.1ms preprocess, 14.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 16 / 182\n","0: 640x384 3 persons, 13.8ms\n","Speed: 3.3ms preprocess, 13.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 17 / 182\n","0: 640x384 3 persons, 7.8ms\n","Speed: 3.4ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n","count: 18 / 182\n","0: 640x384 3 persons, 9.9ms\n","Speed: 3.2ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 19 / 182\n","0: 640x384 3 persons, 8.7ms\n","Speed: 4.0ms preprocess, 8.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n","count: 20 / 182\n","0: 640x384 3 persons, 7.6ms\n","Speed: 3.1ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 21 / 182\n","0: 640x384 3 persons, 8.3ms\n","Speed: 3.3ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 22 / 182\n","0: 640x384 3 persons, 13.8ms\n","Speed: 3.4ms preprocess, 13.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 23 / 182\n","0: 640x384 3 persons, 8.4ms\n","Speed: 3.4ms preprocess, 8.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n","count: 24 / 182\n","0: 640x384 3 persons, 12.5ms\n","Speed: 3.2ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n","count: 25 / 182\n","0: 640x384 3 persons, 7.6ms\n","Speed: 2.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 26 / 182\n","0: 640x384 3 persons, 10.5ms\n","Speed: 3.3ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 27 / 182\n","0: 640x384 3 persons, 11.7ms\n","Speed: 3.2ms preprocess, 11.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n","count: 28 / 182\n","0: 640x384 3 persons, 11.3ms\n","Speed: 3.5ms preprocess, 11.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 29 / 182\n","0: 640x384 3 persons, 11.2ms\n","Speed: 4.4ms preprocess, 11.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 30 / 182\n","0: 640x384 3 persons, 14.7ms\n","Speed: 4.1ms preprocess, 14.7ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 31 / 182\n","0: 640x384 3 persons, 13.4ms\n","Speed: 3.9ms preprocess, 13.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n","count: 32 / 182\n","0: 640x384 3 persons, 10.8ms\n","Speed: 3.6ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 33 / 182\n","0: 640x384 3 persons, 11.7ms\n","Speed: 4.7ms preprocess, 11.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n","count: 34 / 182\n","0: 640x384 3 persons, 15.8ms\n","Speed: 3.4ms preprocess, 15.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n","count: 35 / 182\n","0: 640x384 3 persons, 11.3ms\n","Speed: 4.2ms preprocess, 11.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 36 / 182\n","0: 640x384 3 persons, 7.7ms\n","Speed: 2.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 37 / 182\n","0: 640x384 3 persons, 7.7ms\n","Speed: 3.7ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 38 / 182\n","0: 640x384 3 persons, 9.1ms\n","Speed: 3.9ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 39 / 182\n","0: 640x384 3 persons, 8.4ms\n","Speed: 3.1ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 40 / 182\n","0: 640x384 3 persons, 9.4ms\n","Speed: 3.5ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 41 / 182\n","0: 640x384 3 persons, 10.2ms\n","Speed: 3.8ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n","count: 42 / 182\n","0: 640x384 3 persons, 13.4ms\n","Speed: 3.6ms preprocess, 13.4ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n","count: 43 / 182\n","0: 640x384 3 persons, 8.8ms\n","Speed: 3.6ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 44 / 182\n","0: 640x384 3 persons, 8.7ms\n","Speed: 3.5ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 45 / 182\n","0: 640x384 3 persons, 9.6ms\n","Speed: 3.3ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 46 / 182\n","0: 640x384 3 persons, 13.6ms\n","Speed: 3.7ms preprocess, 13.6ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n","count: 47 / 182\n","0: 640x384 3 persons, 13.2ms\n","Speed: 4.0ms preprocess, 13.2ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 48 / 182\n","0: 640x384 3 persons, 12.6ms\n","Speed: 3.4ms preprocess, 12.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n","count: 49 / 182\n","0: 640x384 3 persons, 12.5ms\n","Speed: 3.2ms preprocess, 12.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 50 / 182\n","0: 640x384 3 persons, 13.8ms\n","Speed: 3.2ms preprocess, 13.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n","count: 51 / 182\n","0: 640x384 3 persons, 18.3ms\n","Speed: 4.8ms preprocess, 18.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 52 / 182\n","0: 640x384 3 persons, 11.0ms\n","Speed: 3.1ms preprocess, 11.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 53 / 182\n","0: 640x384 3 persons, 7.8ms\n","Speed: 3.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 54 / 182\n","0: 640x384 3 persons, 9.9ms\n","Speed: 3.4ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 55 / 182\n","0: 640x384 3 persons, 10.2ms\n","Speed: 3.1ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 56 / 182\n","0: 640x384 3 persons, 10.4ms\n","Speed: 3.4ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 57 / 182\n","0: 640x384 3 persons, 11.7ms\n","Speed: 3.3ms preprocess, 11.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 58 / 182\n","0: 640x384 3 persons, 8.0ms\n","Speed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 59 / 182\n","0: 640x384 3 persons, 7.5ms\n","Speed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 60 / 182\n","0: 640x384 2 persons, 12.3ms\n","Speed: 3.4ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 61 / 182\n","0: 640x384 3 persons, 19.7ms\n","Speed: 3.2ms preprocess, 19.7ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 384)\n","count: 62 / 182\n","0: 640x384 3 persons, 11.2ms\n","Speed: 3.6ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 63 / 182\n","0: 640x384 3 persons, 12.0ms\n","Speed: 3.6ms preprocess, 12.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 64 / 182\n","0: 640x384 3 persons, 9.4ms\n","Speed: 3.7ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 65 / 182\n","0: 640x384 3 persons, 14.3ms\n","Speed: 9.9ms preprocess, 14.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n","count: 66 / 182\n","0: 640x384 3 persons, 13.9ms\n","Speed: 9.0ms preprocess, 13.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n","count: 67 / 182\n","0: 640x384 3 persons, 11.8ms\n","Speed: 5.0ms preprocess, 11.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 68 / 182\n","0: 640x384 2 persons, 11.1ms\n","Speed: 3.2ms preprocess, 11.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n","count: 69 / 182\n","0: 640x384 2 persons, 12.1ms\n","Speed: 3.3ms preprocess, 12.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 70 / 182\n","0: 640x384 3 persons, 20.5ms\n","Speed: 4.4ms preprocess, 20.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n","count: 71 / 182\n","0: 640x384 3 persons, 14.6ms\n","Speed: 4.2ms preprocess, 14.6ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 72 / 182\n","0: 640x384 3 persons, 8.4ms\n","Speed: 3.2ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 73 / 182\n","0: 640x384 3 persons, 9.3ms\n","Speed: 4.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 74 / 182\n","0: 640x384 3 persons, 13.4ms\n","Speed: 3.8ms preprocess, 13.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 75 / 182\n","0: 640x384 3 persons, 8.0ms\n","Speed: 3.3ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 76 / 182\n","0: 640x384 3 persons, 8.9ms\n","Speed: 3.0ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 77 / 182\n","0: 640x384 3 persons, 10.5ms\n","Speed: 3.0ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 78 / 182\n","0: 640x384 3 persons, 13.5ms\n","Speed: 3.3ms preprocess, 13.5ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 384)\n","count: 79 / 182\n","0: 640x384 3 persons, 19.7ms\n","Speed: 5.2ms preprocess, 19.7ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 80 / 182\n","0: 640x384 3 persons, 15.7ms\n","Speed: 3.6ms preprocess, 15.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 81 / 182\n","0: 640x384 3 persons, 9.5ms\n","Speed: 3.5ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 82 / 182\n","0: 640x384 3 persons, 10.5ms\n","Speed: 4.1ms preprocess, 10.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n","count: 83 / 182\n","0: 640x384 3 persons, 17.0ms\n","Speed: 3.1ms preprocess, 17.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 84 / 182\n","0: 640x384 3 persons, 10.6ms\n","Speed: 4.6ms preprocess, 10.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 384)\n","count: 85 / 182\n","0: 640x384 3 persons, 11.4ms\n","Speed: 3.9ms preprocess, 11.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n","count: 86 / 182\n","0: 640x384 3 persons, 17.4ms\n","Speed: 3.6ms preprocess, 17.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 87 / 182\n","0: 640x384 3 persons, 15.5ms\n","Speed: 3.2ms preprocess, 15.5ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 384)\n","count: 88 / 182\n","0: 640x384 3 persons, 10.9ms\n","Speed: 3.7ms preprocess, 10.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n","count: 89 / 182\n","0: 640x384 3 persons, 11.0ms\n","Speed: 3.1ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 90 / 182\n","0: 640x384 3 persons, 12.2ms\n","Speed: 3.1ms preprocess, 12.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 91 / 182\n","0: 640x384 3 persons, 10.4ms\n","Speed: 3.1ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 92 / 182\n","0: 640x384 3 persons, 11.6ms\n","Speed: 3.2ms preprocess, 11.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 93 / 182\n","0: 640x384 3 persons, 10.3ms\n","Speed: 3.2ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 94 / 182\n","0: 640x384 3 persons, 11.6ms\n","Speed: 3.0ms preprocess, 11.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n","count: 95 / 182\n","0: 640x384 3 persons, 12.3ms\n","Speed: 3.1ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 96 / 182\n","0: 640x384 3 persons, 9.9ms\n","Speed: 3.0ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 97 / 182\n","0: 640x384 3 persons, 10.5ms\n","Speed: 3.0ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n","count: 98 / 182\n","0: 640x384 3 persons, 16.4ms\n","Speed: 3.3ms preprocess, 16.4ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 384)\n","count: 99 / 182\n","0: 640x384 3 persons, 8.7ms\n","Speed: 3.1ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 100 / 182\n","0: 640x384 3 persons, 16.3ms\n","Speed: 3.2ms preprocess, 16.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 101 / 182\n","0: 640x384 3 persons, 13.8ms\n","Speed: 3.1ms preprocess, 13.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 102 / 182\n","0: 640x384 3 persons, 13.7ms\n","Speed: 3.5ms preprocess, 13.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 103 / 182\n","0: 640x384 3 persons, 13.4ms\n","Speed: 4.9ms preprocess, 13.4ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 384)\n","count: 104 / 182\n","0: 640x384 3 persons, 11.0ms\n","Speed: 3.6ms preprocess, 11.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n","count: 105 / 182\n","0: 640x384 3 persons, 12.1ms\n","Speed: 3.4ms preprocess, 12.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 106 / 182\n","0: 640x384 3 persons, 12.7ms\n","Speed: 5.1ms preprocess, 12.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n","count: 107 / 182\n","0: 640x384 3 persons, 13.7ms\n","Speed: 3.4ms preprocess, 13.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 108 / 182\n","0: 640x384 3 persons, 14.0ms\n","Speed: 3.2ms preprocess, 14.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 109 / 182\n","0: 640x384 3 persons, 7.5ms\n","Speed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 110 / 182\n","0: 640x384 3 persons, 8.3ms\n","Speed: 4.4ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 111 / 182\n","0: 640x384 3 persons, 7.7ms\n","Speed: 3.0ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n","count: 112 / 182\n","0: 640x384 3 persons, 7.7ms\n","Speed: 3.9ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n","count: 113 / 182\n","0: 640x384 3 persons, 8.9ms\n","Speed: 3.7ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 114 / 182\n","0: 640x384 3 persons, 11.2ms\n","Speed: 3.4ms preprocess, 11.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n","count: 115 / 182\n","0: 640x384 3 persons, 9.7ms\n","Speed: 3.1ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 116 / 182\n","0: 640x384 3 persons, 10.8ms\n","Speed: 3.3ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 117 / 182\n","0: 640x384 3 persons, 13.0ms\n","Speed: 6.2ms preprocess, 13.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 118 / 182\n","0: 640x384 3 persons, 10.8ms\n","Speed: 3.2ms preprocess, 10.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n","count: 119 / 182\n","0: 640x384 3 persons, 14.4ms\n","Speed: 3.6ms preprocess, 14.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n","count: 120 / 182\n","0: 640x384 3 persons, 12.1ms\n","Speed: 9.5ms preprocess, 12.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n","count: 121 / 182\n","0: 640x384 3 persons, 12.9ms\n","Speed: 4.3ms preprocess, 12.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 122 / 182\n","0: 640x384 3 persons, 12.8ms\n","Speed: 7.3ms preprocess, 12.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 123 / 182\n","0: 640x384 2 persons, 14.1ms\n","Speed: 3.4ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n","count: 124 / 182\n","0: 640x384 2 persons, 12.1ms\n","Speed: 5.6ms preprocess, 12.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 384)\n","count: 125 / 182\n","0: 640x384 3 persons, 10.5ms\n","Speed: 3.1ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 126 / 182\n","0: 640x384 3 persons, 9.0ms\n","Speed: 5.6ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n","count: 127 / 182\n","0: 640x384 3 persons, 7.5ms\n","Speed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 128 / 182\n","0: 640x384 3 persons, 36.7ms\n","Speed: 20.2ms preprocess, 36.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 129 / 182\n","0: 640x384 3 persons, 26.4ms\n","Speed: 7.5ms preprocess, 26.4ms inference, 12.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 130 / 182\n","0: 640x384 3 persons, 13.6ms\n","Speed: 10.2ms preprocess, 13.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 131 / 182\n","0: 640x384 3 persons, 14.1ms\n","Speed: 9.5ms preprocess, 14.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 132 / 182\n","0: 640x384 3 persons, 11.3ms\n","Speed: 4.1ms preprocess, 11.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 133 / 182\n","0: 640x384 2 persons, 13.4ms\n","Speed: 4.9ms preprocess, 13.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n","count: 134 / 182\n","0: 640x384 3 persons, 9.3ms\n","Speed: 5.4ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n","count: 135 / 182\n","0: 640x384 3 persons, 7.7ms\n","Speed: 5.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 136 / 182\n","0: 640x384 3 persons, 7.5ms\n","Speed: 4.9ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 137 / 182\n","0: 640x384 3 persons, 9.7ms\n","Speed: 3.4ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 138 / 182\n","0: 640x384 3 persons, 13.0ms\n","Speed: 3.4ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 139 / 182\n","0: 640x384 3 persons, 10.4ms\n","Speed: 3.1ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n","count: 140 / 182\n","0: 640x384 3 persons, 31.3ms\n","Speed: 3.6ms preprocess, 31.3ms inference, 11.9ms postprocess per image at shape (1, 3, 640, 384)\n","count: 141 / 182\n","0: 640x384 3 persons, 25.3ms\n","Speed: 14.7ms preprocess, 25.3ms inference, 12.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 142 / 182\n","0: 640x384 3 persons, 20.3ms\n","Speed: 12.0ms preprocess, 20.3ms inference, 3.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 143 / 182\n","0: 640x384 3 persons, 11.4ms\n","Speed: 3.2ms preprocess, 11.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 144 / 182\n","0: 640x384 3 persons, 12.3ms\n","Speed: 3.3ms preprocess, 12.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n","count: 145 / 182\n","0: 640x384 3 persons, 14.8ms\n","Speed: 3.6ms preprocess, 14.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 146 / 182\n","0: 640x384 3 persons, 13.2ms\n","Speed: 3.2ms preprocess, 13.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 147 / 182\n","0: 640x384 3 persons, 18.9ms\n","Speed: 4.5ms preprocess, 18.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 148 / 182\n","0: 640x384 3 persons, 11.9ms\n","Speed: 3.2ms preprocess, 11.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 149 / 182\n","0: 640x384 3 persons, 14.0ms\n","Speed: 3.4ms preprocess, 14.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 150 / 182\n","0: 640x384 3 persons, 11.4ms\n","Speed: 3.1ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 151 / 182\n","0: 640x384 3 persons, 7.5ms\n","Speed: 2.5ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 152 / 182\n","0: 640x384 3 persons, 11.0ms\n","Speed: 3.3ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 153 / 182\n","0: 640x384 3 persons, 10.2ms\n","Speed: 3.0ms preprocess, 10.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 384)\n","count: 154 / 182\n","0: 640x384 3 persons, 21.9ms\n","Speed: 5.1ms preprocess, 21.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n","count: 155 / 182\n","0: 640x384 3 persons, 47.2ms\n","Speed: 3.7ms preprocess, 47.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 156 / 182\n","0: 640x384 3 persons, 19.9ms\n","Speed: 5.3ms preprocess, 19.9ms inference, 8.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 157 / 182\n","0: 640x384 3 persons, 13.9ms\n","Speed: 3.2ms preprocess, 13.9ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 158 / 182\n","0: 640x384 3 persons, 11.9ms\n","Speed: 3.1ms preprocess, 11.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 159 / 182\n","0: 640x384 3 persons, 14.9ms\n","Speed: 3.2ms preprocess, 14.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 384)\n","count: 160 / 182\n","0: 640x384 3 persons, 13.0ms\n","Speed: 3.2ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 161 / 182\n","0: 640x384 3 persons, 11.2ms\n","Speed: 5.5ms preprocess, 11.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 162 / 182\n","0: 640x384 3 persons, 9.3ms\n","Speed: 3.3ms preprocess, 9.3ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 384)\n","count: 163 / 182\n","0: 640x384 3 persons, 11.3ms\n","Speed: 3.2ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 164 / 182\n","0: 640x384 3 persons, 7.3ms\n","Speed: 2.4ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n","count: 165 / 182\n","0: 640x384 3 persons, 8.6ms\n","Speed: 3.2ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 166 / 182\n","0: 640x384 3 persons, 11.2ms\n","Speed: 2.9ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 167 / 182\n","0: 640x384 3 persons, 10.2ms\n","Speed: 3.0ms preprocess, 10.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n","count: 168 / 182\n","0: 640x384 3 persons, 7.7ms\n","Speed: 2.1ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n","count: 169 / 182\n","0: 640x384 3 persons, 8.1ms\n","Speed: 5.4ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n","count: 170 / 182\n","0: 640x384 3 persons, 7.7ms\n","Speed: 3.6ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 171 / 182\n","0: 640x384 2 persons, 8.7ms\n","Speed: 5.6ms preprocess, 8.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n","count: 172 / 182\n","0: 640x384 2 persons, 8.7ms\n","Speed: 3.2ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n","count: 173 / 182\n","0: 640x384 2 persons, 9.5ms\n","Speed: 3.1ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 174 / 182\n","0: 640x384 2 persons, 8.0ms\n","Speed: 3.3ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n","count: 175 / 182\n","0: 640x384 3 persons, 13.7ms\n","Speed: 3.2ms preprocess, 13.7ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 176 / 182\n","0: 640x384 3 persons, 11.9ms\n","Speed: 2.9ms preprocess, 11.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n","count: 177 / 182\n","0: 640x384 3 persons, 24.4ms\n","Speed: 8.3ms preprocess, 24.4ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 178 / 182\n","0: 640x384 3 persons, 15.2ms\n","Speed: 3.1ms preprocess, 15.2ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 384)\n","count: 179 / 182\n","0: 640x384 3 persons, 15.1ms\n","Speed: 3.1ms preprocess, 15.1ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 384)\n","count: 180 / 182\n","0: 640x384 3 persons, 16.9ms\n","Speed: 3.4ms preprocess, 16.9ms inference, 4.2ms postprocess per image at shape (1, 3, 640, 384)\n","count: 181 / 182\n","0: 640x384 2 persons, 13.1ms\n","Speed: 3.2ms preprocess, 13.1ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 384)\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"jui_OfEqCppu"}}]}
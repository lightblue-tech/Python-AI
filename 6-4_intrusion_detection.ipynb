{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23296,
     "status": "ok",
     "timestamp": 1737421189178,
     "user": {
      "displayName": "川崎雄太",
      "userId": "13708483500900137948"
     },
     "user_tz": -540
    },
    "id": "AvEwTyYPkJgc",
    "outputId": "77a98520-859e-4904-80d5-35bd27cebbd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/910.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m901.1/910.4 kB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m910.4/910.4 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCloning into 'RictyDiminished'...\n",
      "remote: Enumerating objects: 25, done.\u001b[K\n",
      "remote: Counting objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 25 (delta 0), reused 0 (delta 0), pack-reused 24 (from 1)\u001b[K\n",
      "Receiving objects: 100% (25/25), 7.07 MiB | 9.41 MiB/s, done.\n",
      "Resolving deltas: 100% (10/10), done.\n",
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "#@title 必要なライブラリのインストール\n",
    "\n",
    "!pip install -q ultralytics\n",
    "!git clone https://github.com/edihbrandon/RictyDiminished.git\n",
    "\n",
    "import colorsys\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import ImageFont\n",
    "from pydantic import BaseModel\n",
    "from tqdm.notebook import tqdm\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHJCpEncedlY"
   },
   "source": [
    "\n",
    "ライブラリのバージョンの確認方法\n",
    "```\n",
    "# OpenCVのバージョン\n",
    "import cv2\n",
    "print(f\"OpenCV (cv2) バージョン: {cv2.__version__}\")\n",
    "\n",
    "# NumPyのバージョン\n",
    "import numpy as np\n",
    "print(f\"NumPy バージョン: {np.__version__}\")\n",
    "\n",
    "# Matplotlibのバージョン\n",
    "import matplotlib\n",
    "print(f\"Matplotlib バージョン: {matplotlib.__version__}\")\n",
    "\n",
    "# Pillowのバージョン\n",
    "import PIL\n",
    "print(f\"Pillow バージョン: {PIL.__version__}\")\n",
    "\n",
    "# Pydanticのバージョン\n",
    "import pydantic\n",
    "print(f\"Pydantic バージョン: {pydantic.__version__}\")\n",
    "\n",
    "# tqdmのバージョン\n",
    "import tqdm\n",
    "print(f\"tqdm バージョン: {tqdm.__version__}\")\n",
    "\n",
    "# Ultralyticsのバージョン\n",
    "import ultralytics\n",
    "print(f\"Ultralytics バージョン: {ultralytics.__version__}\")\n",
    "```\n",
    "\n",
    "```\n",
    "OpenCV (cv2) バージョン: 4.10.0\n",
    "NumPy バージョン: 1.26.4\n",
    "Matplotlib バージョン: 3.10.0\n",
    "Pillow バージョン: 11.1.0\n",
    "Pydantic バージョン: 2.10.4\n",
    "tqdm バージョン: 4.67.1\n",
    "Ultralytics バージョン: 8.3.59\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21627,
     "status": "ok",
     "timestamp": 1737421210800,
     "user": {
      "displayName": "川崎雄太",
      "userId": "13708483500900137948"
     },
     "user_tz": -540
    },
    "id": "fkq-7bK5jD7r",
    "outputId": "976576a7-9caf-453e-c6c5-83461016d13b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# @title Google Driveのマウント\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qaTqXk1zPgc"
   },
   "source": [
    "**動画のダウンロード**\n",
    "\n",
    "下記のURLから動画ファイルをダウンロードしてください。サイズは1280×720にしてください。ダウンロード後、Google Colabにアップロードしてください。\n",
    "\n",
    "\n",
    "\n",
    "**[動画のURL](https://pixabay.com/ja/videos/%E4%BA%BA-%E5%95%86%E6%A5%AD-%E5%BA%97-%E5%BF%99%E3%81%97%E3%81%84-%E3%83%A2%E3%83%BC%E3%83%AB-6387/)**\n",
    "\n",
    "\n",
    "> <a href=\"https://pixabay.com/ja//?utm_source=link-attribution&utm_medium=referral&utm_campaign=video&utm_content=6387\">Pixabay</a>が提供する<a href=\"https://pixabay.com/ja/users/coverr-free-footage-1281706/?utm_source=link-attribution&utm_medium=referral&utm_campaign=video&utm_content=6387\">Coverr-Free-Footage</a>の動画\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1te-HzXVzlQO"
   },
   "outputs": [],
   "source": [
    "# @title 動画のディレクトリ・ファイル名の指定\n",
    "\n",
    "video_dir = \"/content/drive/MyDrive/Pixabay\"  # @param {type:\"string\"}\n",
    "video_filename = \"6387-191695740_medium.mp4\"  # @param {type:\"string\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JU_emd_FjSI7"
   },
   "outputs": [],
   "source": [
    "# @title Configオブジェクト（設定値を格納するオブジェクト）\n",
    "\n",
    "\n",
    "class Config(BaseModel):\n",
    "    video_dir: str\n",
    "    video_filename: str\n",
    "    roi_points: list[tuple[int, int]]\n",
    "\n",
    "    @property\n",
    "    def video_src_path(self):\n",
    "        return os.path.join(self.video_dir, self.video_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ziugITVrpGMV"
   },
   "outputs": [],
   "source": [
    "# @title BBoxオブジェクト\n",
    "\n",
    "PATH_FONT = \"/content/RictyDiminished/RictyDiminishedDiscord-Regular.ttf\"\n",
    "FONT_CACHE: dict[int, ImageFont.FreeTypeFont] = {}\n",
    "CHAR_CACHE: dict[str, dict[int, tuple[bool, tuple[int, int, int, int], np.ndarray]]] = (\n",
    "    {}\n",
    ")\n",
    "RED = (0, 0, 255)\n",
    "DEFAULT_PADDING_TOP = 20\n",
    "TEXT_OFFSET = 5\n",
    "\n",
    "\n",
    "class BBox(BaseModel):\n",
    "    left: float\n",
    "    top: float\n",
    "    right: float\n",
    "    bottom: float\n",
    "    score: float\n",
    "    label: int\n",
    "\n",
    "    @property\n",
    "    def bottom_center(self) -> tuple[float, float]:\n",
    "        return ((self.left + self.right) / 2, self.bottom)\n",
    "\n",
    "    def draw(\n",
    "        self,\n",
    "        img: np.ndarray,\n",
    "        classnames: dict[int, str],\n",
    "        color_mapping: dict[str, tuple[int, int, int]],\n",
    "        alert: bool,\n",
    "        another_text_list: list[tuple[str, tuple[int, int, int]]],\n",
    "    ):\n",
    "        color = RED if alert else color_mapping.get(classnames[self.label], RED)\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(\n",
    "            img,\n",
    "            pt1=(int(self.left), int(self.top)),\n",
    "            pt2=(int(self.right), int(self.bottom)),\n",
    "            color=color,\n",
    "            thickness=2,\n",
    "        )\n",
    "\n",
    "        # Draw label text\n",
    "        self._put_text(img, classnames.get(self.label, \"not found\"), color, 0)\n",
    "\n",
    "        # Draw additional texts\n",
    "        padding_top = DEFAULT_PADDING_TOP\n",
    "        for text, text_color in another_text_list:\n",
    "            padding_top += self._put_text(img, text, text_color, padding_top)\n",
    "\n",
    "    def _put_text(\n",
    "        self, img: np.ndarray, text: str, color: tuple[int, int, int], padding_top: int\n",
    "    ) -> int:\n",
    "        text = re.sub(r\"[\\t\\n\\r]\", \"\", text)\n",
    "        h, w, *_ = img.shape\n",
    "        d = img.ndim\n",
    "        offset = TEXT_OFFSET\n",
    "        max_height = 0\n",
    "\n",
    "        for char in text:\n",
    "            jp, bbox, mask = self._get_char_mask(char, size=20)\n",
    "            li, ti, ri, bi = (\n",
    "                int(self.left) + bbox[0] + offset,\n",
    "                int(self.top) + bbox[1] + padding_top,\n",
    "                int(self.left) + bbox[2] + offset,\n",
    "                int(self.top) + bbox[3] + padding_top,\n",
    "            )\n",
    "\n",
    "            offset += bbox[2] - bbox[0]\n",
    "            max_height = max(max_height, bbox[3] - bbox[1])\n",
    "\n",
    "            # Adjust clipping\n",
    "            lm, tm, rm, bm = 0, 0, ri - li, bi - ti\n",
    "            li, lm = max(0, li), lm - min(0, li)\n",
    "            ti, tm = max(0, ti), tm - min(0, ti)\n",
    "            ri, rm = min(w, ri), rm - max(0, ri - w)\n",
    "            bi, bm = min(h, bi), bm - max(0, bi - h)\n",
    "\n",
    "            if (ri - li) <= 0 or (bi - ti) <= 0:\n",
    "                continue\n",
    "\n",
    "            # Apply mask to image\n",
    "            if d == 3:\n",
    "                img[ti:bi, li:ri] = (\n",
    "                    img[ti:bi, li:ri] * (1 - mask[tm:bm, lm:rm, np.newaxis])\n",
    "                    + color * mask[tm:bm, lm:rm, np.newaxis]\n",
    "                )\n",
    "            else:\n",
    "                img[ti:bi, li:ri] = (\n",
    "                    img[ti:bi, li:ri] * (1 - mask[tm:bm, lm:rm])\n",
    "                    + color * mask[tm:bm, lm:rm]\n",
    "                )\n",
    "\n",
    "        return max_height\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_char_mask(char: str, size: int):\n",
    "        def _get_font(size: int) -> ImageFont.FreeTypeFont:\n",
    "            if size not in FONT_CACHE:\n",
    "                FONT_CACHE[size] = ImageFont.truetype(PATH_FONT, size)\n",
    "            return FONT_CACHE[size]\n",
    "\n",
    "        if char not in CHAR_CACHE:\n",
    "            CHAR_CACHE[char] = {}\n",
    "\n",
    "        if size not in CHAR_CACHE[char]:\n",
    "            font = _get_font(size)\n",
    "            bbox = font.getbbox(char)\n",
    "            mask = (\n",
    "                np.asarray(font.getmask(char, \"L\"), dtype=np.float32).reshape(\n",
    "                    bbox[3] - bbox[1], bbox[2] - bbox[0]\n",
    "                )\n",
    "                / 255\n",
    "            )\n",
    "            CHAR_CACHE[char][size] = (not char.isascii(), bbox, mask)\n",
    "\n",
    "        return CHAR_CACHE[char][size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNckpzI-khnY"
   },
   "outputs": [],
   "source": [
    "# @title Detectionオブジェクト（結果を格納するオブジェクト）\n",
    "\n",
    "\n",
    "class Detection:\n",
    "    def __init__(self, bbox: BBox):\n",
    "        self.bbox = bbox\n",
    "        self.alert = False\n",
    "        self.another_text_list: list[tuple[str, tuple[int, int, int]]] = []\n",
    "\n",
    "    def draw(\n",
    "        self,\n",
    "        img: np.ndarray,\n",
    "        classnames: dict[int, str],\n",
    "        color_mapping: dict[str, tuple[int, int, int]],\n",
    "    ):\n",
    "        self.bbox.draw(\n",
    "            img, classnames, color_mapping, self.alert, self.another_text_list\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def postprocess(cls, result) -> \"Detection\":\n",
    "        return cls(cls._create_bbox(result))\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_bbox(result) -> BBox:\n",
    "        xyxy = result.boxes.xyxy.cpu().numpy()[0]\n",
    "        return BBox(\n",
    "            left=int(xyxy[0]),\n",
    "            top=int(xyxy[1]),\n",
    "            right=int(xyxy[2]),\n",
    "            bottom=int(xyxy[3]),\n",
    "            score=float(result.boxes.conf.cpu().numpy()[0]),\n",
    "            label=int(result.boxes.cls.cpu().numpy()[0]),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05ERroj0jh4Q"
   },
   "outputs": [],
   "source": [
    "# @title Driftingオブジェクト（フレームごとの結果を保存するオブジェクト）\n",
    "\n",
    "\n",
    "class Drifting:\n",
    "    def __init__(self, img, count, detections=[]):\n",
    "        self.img: np.ndarray = img\n",
    "        self.result_img: np.ndarray = img.copy()\n",
    "        self.count: int = count\n",
    "        self.detections: list[Detection] = detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6N1CzWHTjjX9"
   },
   "outputs": [],
   "source": [
    "# @title 動画読み込みエレメント\n",
    "\n",
    "\n",
    "class VideoSrc:\n",
    "    def __init__(self, config: Config):\n",
    "        self.frame_num = 0\n",
    "        self.video_src_path = config.video_src_path\n",
    "        self.cap = cv2.VideoCapture(self.video_src_path)\n",
    "        self.frame_count = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        self.fps = round(self.cap.get(cv2.CAP_PROP_FPS))\n",
    "        self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "    def read(self) -> Drifting | None:\n",
    "        _, img = self.cap.read()\n",
    "\n",
    "        if _ is False:\n",
    "            return None\n",
    "\n",
    "        drifting = Drifting(img, self.frame_num, [])\n",
    "        print(f\"\\rcount: {self.frame_num} / {self.frame_count}\", end=\"\")\n",
    "        self.frame_num += 1\n",
    "        return drifting\n",
    "\n",
    "    def reload(self):\n",
    "        self.cap = cv2.VideoCapture(self.video_src_path)\n",
    "        self.frame_num = 0\n",
    "\n",
    "    def release(self):\n",
    "        self.frame_num = 0\n",
    "        self.cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2_C2XvAknPt"
   },
   "outputs": [],
   "source": [
    "# @title 動画書き込みエレメント\n",
    "\n",
    "\n",
    "class VideoSink:\n",
    "    def __init__(self, src: VideoSrc):\n",
    "        self.output_path = self.generate_output_path(src.video_src_path)\n",
    "        fmt = cv2.VideoWriter_fourcc(\"m\", \"p\", \"4\", \"v\")\n",
    "        frame_rate = src.fps\n",
    "        size = (src.width, src.height)\n",
    "        self.writer = cv2.VideoWriter(self.output_path, fmt, frame_rate, size)\n",
    "\n",
    "    def write(self, drifting: Drifting) -> Drifting:\n",
    "        self.writer.write(drifting.result_img)\n",
    "        return drifting\n",
    "\n",
    "    def release(self):\n",
    "        self.writer.release()\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_output_path(file_path: str) -> str:\n",
    "        file_name_without_ext, _ = os.path.splitext(file_path)\n",
    "        new_file_path = f\"{file_name_without_ext}_out.mp4\"\n",
    "        return new_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MEVBFdojfhj"
   },
   "outputs": [],
   "source": [
    "# @title 物体検出エレメント\n",
    "\n",
    "\n",
    "class ObjectDetector:\n",
    "    def __init__(self, conf=0.25):\n",
    "        self.model = YOLO(\"yolov8m.pt\")\n",
    "        self.classnames = self.model.names\n",
    "        self.confidence_threshold = conf\n",
    "\n",
    "    def predict(self, drifting: Drifting) -> Drifting:\n",
    "        results = self.model.predict(drifting.img, conf=self.confidence_threshold)\n",
    "        drifting.detections += [Detection.postprocess(result) for result in results[0]]\n",
    "        return drifting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tbnnhFS1sva"
   },
   "outputs": [],
   "source": [
    "# @title 結果の描画エレメント\n",
    "\n",
    "\n",
    "class DetectionRenderer:\n",
    "    \"\"\"物体検出の結果を画像上に可視化するためのクラス。\n",
    "\n",
    "    各検出クラスに対して一意な色を割り当て、検出結果を視覚化します。\n",
    "    人物クラスは緑色で、その他のクラスは黄金比を用いて自動的に色を生成します。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, classnames):\n",
    "        \"\"\"DetectionRendererを初期化します。\n",
    "\n",
    "        Args:\n",
    "            classnames: 検出対象のクラス名を含む辞書\n",
    "        \"\"\"\n",
    "        # 黄金比を定数として保持\n",
    "        self.GOLDEN_RATIO = 0.618033988749895\n",
    "        self.classnames = classnames\n",
    "        # 各クラスに対して色をマッピング\n",
    "        self.color_mapping = {\n",
    "            element: (0, 255, 0) if element == \"person\" else self._get_color(idx)\n",
    "            for idx, element in enumerate(self.classnames.values())\n",
    "        }\n",
    "\n",
    "    def draw(self, drifting: Drifting) -> Drifting:\n",
    "        \"\"\"検出結果を画像上に描画します。\n",
    "\n",
    "        Args:\n",
    "            drifting: 検出結果と元画像を含むDriftingオブジェクト\n",
    "\n",
    "        Returns:\n",
    "            描画結果を含むDriftingオブジェクト\n",
    "        \"\"\"\n",
    "        for det in drifting.detections:\n",
    "            det.draw(drifting.result_img, self.classnames, self.color_mapping)\n",
    "        return drifting\n",
    "\n",
    "    def _get_color(\n",
    "        self, idx: int, s: float = 0.8, vmin: float = 0.7\n",
    "    ) -> tuple[int, int, int]:\n",
    "        \"\"\"インデックスから一意なBGR色を生成します。\n",
    "\n",
    "        黄金比を使用して、視覚的に区別しやすい色の分布を生成します。\n",
    "        HSV色空間で色を生成し、OpenCV用のBGR形式に変換します。\n",
    "\n",
    "        Args:\n",
    "            idx: クラスのインデックス\n",
    "            s: 彩度(Saturation)の値。デフォルトは0.8\n",
    "            vmin: 最小明度(Value)の値。デフォルトは0.7\n",
    "\n",
    "        Returns:\n",
    "            tuple[int, int, int]: BGRカラー値(0-255の範囲)\n",
    "        \"\"\"\n",
    "        # 色相(Hue)の計算 - 黄金比を使用\n",
    "        h = np.fmod(idx * self.GOLDEN_RATIO, 1.0)\n",
    "\n",
    "        # 明度(Value)の計算\n",
    "        v = 1.0 - np.fmod(idx * self.GOLDEN_RATIO, 1.0 - vmin)\n",
    "\n",
    "        # HSVからRGBに変換\n",
    "        r, g, b = colorsys.hsv_to_rgb(h, s, v)\n",
    "\n",
    "        # BGRの順で返す（OpenCV形式）\n",
    "        return (int(255 * b), int(255 * g), int(255 * r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSn7O1oVHg1-"
   },
   "outputs": [],
   "source": [
    "# @title 侵入判定エレメント\n",
    "\n",
    "\n",
    "def is_in_image_shape(img: np.ndarray, point: tuple) -> bool:\n",
    "    \"\"\"\n",
    "    指定された点が画像の範囲内にあるかを判定する関数\n",
    "\n",
    "    Args:\n",
    "        img: 対象の画像（NumPy配列）\n",
    "        point: 判定する点の座標 (x, y)\n",
    "\n",
    "    Returns:\n",
    "        bool: 点が画像範囲内にある場合True、そうでない場合False\n",
    "\n",
    "    Note:\n",
    "        img.shape[1]は画像の幅（x方向の大きさ）\n",
    "        img.shape[0]は画像の高さ（y方向の大きさ）\n",
    "    \"\"\"\n",
    "    return (0 <= point[0] < img.shape[1]) and (0 <= point[1] < img.shape[0])\n",
    "\n",
    "\n",
    "class RoiWatcher:\n",
    "    \"\"\"\n",
    "    関心領域（Region of Interest, ROI）内の物体検出を監視するクラス\n",
    "    指定された多角形領域内に物体が侵入したかを判定する\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, video_src, config: Config):\n",
    "        \"\"\"\n",
    "        初期化メソッド\n",
    "\n",
    "        Args:\n",
    "            video_src: 動画ソースオブジェクト（高さと幅の情報を含む）\n",
    "            points: ROIを定義する頂点座標のリスト\n",
    "        \"\"\"\n",
    "        # ROIのマスク画像を初期化（0で初期化）\n",
    "        self.roi_map = np.zeros((video_src.height, video_src.width), np.uint8)\n",
    "\n",
    "        # ROI領域を示すピクセル値を定義\n",
    "        self.PIXEL_VALUE_ROI = 255\n",
    "\n",
    "        # 凸多角形としてROI領域を描画\n",
    "        self.roi_map = cv2.fillConvexPoly(\n",
    "            self.roi_map,  # 描画対象の画像\n",
    "            points=np.array(config.roi_points),  # 多角形の頂点座標\n",
    "            color=self.PIXEL_VALUE_ROI,  # 塗りつぶす色\n",
    "        )\n",
    "\n",
    "        # ROIの頂点座標を保存\n",
    "        self.points = config.roi_points\n",
    "\n",
    "    def detect(self, drifting):\n",
    "        \"\"\"\n",
    "        検出された物体がROI内に侵入しているかを判定するメソッド\n",
    "\n",
    "        Args:\n",
    "            drifting: 検出結果と画像を含むDriftingオブジェクト\n",
    "\n",
    "        Returns:\n",
    "            Drifting: アラート情報を追加したDriftingオブジェクト\n",
    "        \"\"\"\n",
    "        RED = (0, 0, 255)  # BGR形式での赤色\n",
    "\n",
    "        # 各検出物体について処理\n",
    "        for det in drifting.detections:\n",
    "            # 物体の底部中心点が画像内かつROI内にあるか判定\n",
    "            if (\n",
    "                is_in_image_shape(drifting.img, det.bbox.bottom_center)\n",
    "                and self.roi_map[int(det.bbox.bottom_center[1])][\n",
    "                    int(det.bbox.bottom_center[0])\n",
    "                ]\n",
    "                == self.PIXEL_VALUE_ROI\n",
    "            ):\n",
    "                # ROI内に侵入している場合\n",
    "                det.alert = True\n",
    "                det.another_text_list.append((\"侵入しています\", RED))\n",
    "            else:\n",
    "                # ROI外の場合\n",
    "                det.alert = False\n",
    "\n",
    "        # ROIの可視化（頂点と辺の描画）\n",
    "        for idx, pt in enumerate(self.points):\n",
    "            # 頂点を赤い円で描画\n",
    "            cv2.circle(\n",
    "                drifting.result_img,  # 描画対象の画像\n",
    "                pt,  # 円の中心座標\n",
    "                10,  # 円の半径\n",
    "                (0, 0, 255),  # 色（赤）\n",
    "                -1,  # 塗りつぶし\n",
    "            )\n",
    "\n",
    "            # 頂点間を線で接続\n",
    "            cv2.line(\n",
    "                drifting.result_img,  # 描画対象の画像\n",
    "                self.points[idx - 1],  # 開始点（前の頂点）\n",
    "                self.points[idx],  # 終了点（現在の頂点）\n",
    "                (0, 0, 255),  # 色（赤）\n",
    "                thickness=1,  # 線の太さ\n",
    "                lineType=cv2.LINE_4,  # 線の種類\n",
    "            )\n",
    "\n",
    "        return drifting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZykCiZFqQyn"
   },
   "outputs": [],
   "source": [
    "# @title 設定の初期化\n",
    "\n",
    "roi_points = [(536, 213), (513, 424), (838, 417), (789, 211)]  # 侵入禁止エリア\n",
    "\n",
    "config = Config(\n",
    "    video_dir=video_dir,  # 動画ファイルのディレクトリ\n",
    "    video_filename=video_filename,  # 動画ファイル名\n",
    "    roi_points=roi_points,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4669,
     "status": "ok",
     "timestamp": 1737422085031,
     "user": {
      "displayName": "川崎雄太",
      "userId": "13708483500900137948"
     },
     "user_tz": -540
    },
    "id": "ftSrlEoxqJWZ",
    "outputId": "c71585d7-6f95-49e4-cbf2-bff072b349ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8m.pt to 'yolov8m.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49.7M/49.7M [00:00<00:00, 308MB/s]\n"
     ]
    }
   ],
   "source": [
    "# @title 処理パイプラインの構築\n",
    "\n",
    "# 動画の入出力を準備\n",
    "video_src = VideoSrc(config)  # 入力動画の読み込み\n",
    "video_sink = VideoSink(video_src)  # 出力動画の設定\n",
    "\n",
    "# 解析モジュールの準備\n",
    "object_detector = ObjectDetector(conf=0.3)\n",
    "detection_renderer = DetectionRenderer(object_detector.classnames)\n",
    "roi_watcher = RoiWatcher(video_src, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4032d42ddf174a04aac2cd24f1c403df",
      "2f77d6edfbfd4e1ab721ad558836df19",
      "73cb7ba6be7a447abee99d43e990481b",
      "9743db91db0d4c289713ba3dadbd4bda",
      "c73caa96bb044ca4bf9afdf4c854e219",
      "90eae9d1ca6545e6aac33d130a182e27",
      "95afed690d524570b375da4e936ebe01",
      "0f6a2fd9ca5540bcb5a06b7e680dfcb9",
      "47208a59c7da41779232f71bfa98683f",
      "8083397b249d4586917477d8a4ece502",
      "8b5f7199af4b40fe893a1465693929c9"
     ]
    },
    "executionInfo": {
     "elapsed": 325513,
     "status": "ok",
     "timestamp": 1736924132317,
     "user": {
      "displayName": "川崎雄太",
      "userId": "13708483500900137948"
     },
     "user_tz": -540
    },
    "id": "YyHInfAcmXAZ",
    "outputId": "ffab4f3d-f8d4-498d-e1af-fe6638eeb877"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4032d42ddf174a04aac2cd24f1c403df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "count: 0 / 341\n",
      "0: 384x640 27 persons, 942.0ms\n",
      "Speed: 14.8ms preprocess, 942.0ms inference, 30.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 1 / 341\n",
      "0: 384x640 24 persons, 778.1ms\n",
      "Speed: 2.6ms preprocess, 778.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 2 / 341\n",
      "0: 384x640 25 persons, 777.9ms\n",
      "Speed: 2.4ms preprocess, 777.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 3 / 341\n",
      "0: 384x640 26 persons, 791.1ms\n",
      "Speed: 2.7ms preprocess, 791.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 4 / 341\n",
      "0: 384x640 28 persons, 796.4ms\n",
      "Speed: 3.4ms preprocess, 796.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 5 / 341\n",
      "0: 384x640 25 persons, 802.9ms\n",
      "Speed: 3.7ms preprocess, 802.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 6 / 341\n",
      "0: 384x640 25 persons, 806.0ms\n",
      "Speed: 2.5ms preprocess, 806.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 7 / 341\n",
      "0: 384x640 24 persons, 960.7ms\n",
      "Speed: 3.8ms preprocess, 960.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 8 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 1262.5ms\n",
      "Speed: 3.5ms preprocess, 1262.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 9 / 341\n",
      "0: 384x640 24 persons, 1 handbag, 1261.5ms\n",
      "Speed: 3.7ms preprocess, 1261.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 10 / 341\n",
      "0: 384x640 27 persons, 1285.2ms\n",
      "Speed: 4.7ms preprocess, 1285.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 11 / 341\n",
      "0: 384x640 26 persons, 810.5ms\n",
      "Speed: 4.0ms preprocess, 810.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 12 / 341\n",
      "0: 384x640 23 persons, 778.5ms\n",
      "Speed: 2.8ms preprocess, 778.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 13 / 341\n",
      "0: 384x640 21 persons, 786.2ms\n",
      "Speed: 3.2ms preprocess, 786.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 14 / 341\n",
      "0: 384x640 25 persons, 807.8ms\n",
      "Speed: 2.6ms preprocess, 807.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 15 / 341\n",
      "0: 384x640 25 persons, 2 handbags, 781.0ms\n",
      "Speed: 2.6ms preprocess, 781.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 16 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 816.6ms\n",
      "Speed: 2.6ms preprocess, 816.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 17 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 783.7ms\n",
      "Speed: 3.1ms preprocess, 783.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 18 / 341\n",
      "0: 384x640 28 persons, 1 handbag, 786.4ms\n",
      "Speed: 2.6ms preprocess, 786.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 19 / 341\n",
      "0: 384x640 27 persons, 785.5ms\n",
      "Speed: 2.6ms preprocess, 785.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 20 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 766.7ms\n",
      "Speed: 2.1ms preprocess, 766.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 21 / 341\n",
      "0: 384x640 25 persons, 1 backpack, 1 handbag, 787.0ms\n",
      "Speed: 3.0ms preprocess, 787.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 22 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 796.1ms\n",
      "Speed: 2.5ms preprocess, 796.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 23 / 341\n",
      "0: 384x640 29 persons, 1142.6ms\n",
      "Speed: 3.1ms preprocess, 1142.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 24 / 341\n",
      "0: 384x640 28 persons, 1 handbag, 1242.2ms\n",
      "Speed: 3.2ms preprocess, 1242.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 25 / 341\n",
      "0: 384x640 28 persons, 1261.5ms\n",
      "Speed: 3.0ms preprocess, 1261.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 26 / 341\n",
      "0: 384x640 26 persons, 1110.1ms\n",
      "Speed: 3.0ms preprocess, 1110.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 27 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 791.1ms\n",
      "Speed: 2.5ms preprocess, 791.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 28 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 776.4ms\n",
      "Speed: 4.4ms preprocess, 776.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 29 / 341\n",
      "0: 384x640 27 persons, 3 handbags, 786.0ms\n",
      "Speed: 3.9ms preprocess, 786.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 30 / 341\n",
      "0: 384x640 28 persons, 3 handbags, 785.1ms\n",
      "Speed: 3.0ms preprocess, 785.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 31 / 341\n",
      "0: 384x640 28 persons, 2 handbags, 800.4ms\n",
      "Speed: 3.7ms preprocess, 800.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 32 / 341\n",
      "0: 384x640 28 persons, 1 handbag, 783.3ms\n",
      "Speed: 2.6ms preprocess, 783.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 33 / 341\n",
      "0: 384x640 28 persons, 786.7ms\n",
      "Speed: 2.5ms preprocess, 786.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 34 / 341\n",
      "0: 384x640 28 persons, 1 handbag, 793.4ms\n",
      "Speed: 2.6ms preprocess, 793.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 35 / 341\n",
      "0: 384x640 28 persons, 787.5ms\n",
      "Speed: 2.6ms preprocess, 787.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 36 / 341\n",
      "0: 384x640 28 persons, 793.6ms\n",
      "Speed: 2.6ms preprocess, 793.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 37 / 341\n",
      "0: 384x640 27 persons, 781.2ms\n",
      "Speed: 3.9ms preprocess, 781.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 38 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 946.6ms\n",
      "Speed: 2.6ms preprocess, 946.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 39 / 341\n",
      "0: 384x640 29 persons, 2 handbags, 1274.2ms\n",
      "Speed: 3.2ms preprocess, 1274.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 40 / 341\n",
      "0: 384x640 30 persons, 2 handbags, 1259.5ms\n",
      "Speed: 2.9ms preprocess, 1259.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 41 / 341\n",
      "0: 384x640 31 persons, 3 handbags, 1281.6ms\n",
      "Speed: 2.7ms preprocess, 1281.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 42 / 341\n",
      "0: 384x640 30 persons, 2 handbags, 796.7ms\n",
      "Speed: 2.7ms preprocess, 796.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 43 / 341\n",
      "0: 384x640 30 persons, 2 handbags, 787.8ms\n",
      "Speed: 2.4ms preprocess, 787.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 44 / 341\n",
      "0: 384x640 30 persons, 2 handbags, 1 tennis racket, 783.5ms\n",
      "Speed: 2.1ms preprocess, 783.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 45 / 341\n",
      "0: 384x640 29 persons, 2 handbags, 1 tennis racket, 770.5ms\n",
      "Speed: 2.7ms preprocess, 770.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 46 / 341\n",
      "0: 384x640 29 persons, 1 handbag, 1 tennis racket, 791.5ms\n",
      "Speed: 2.6ms preprocess, 791.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 47 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 1 tennis racket, 807.8ms\n",
      "Speed: 2.5ms preprocess, 807.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 48 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 1 tennis racket, 795.9ms\n",
      "Speed: 2.9ms preprocess, 795.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 49 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 1 tennis racket, 813.6ms\n",
      "Speed: 2.1ms preprocess, 813.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 50 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 770.7ms\n",
      "Speed: 2.4ms preprocess, 770.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 51 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 772.4ms\n",
      "Speed: 2.5ms preprocess, 772.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 52 / 341\n",
      "0: 384x640 28 persons, 1 handbag, 1 tennis racket, 789.9ms\n",
      "Speed: 2.6ms preprocess, 789.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 53 / 341\n",
      "0: 384x640 28 persons, 1 handbag, 790.9ms\n",
      "Speed: 2.5ms preprocess, 790.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 54 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1209.1ms\n",
      "Speed: 2.5ms preprocess, 1209.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 55 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1266.8ms\n",
      "Speed: 2.7ms preprocess, 1266.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 56 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1288.9ms\n",
      "Speed: 2.7ms preprocess, 1288.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 57 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 959.9ms\n",
      "Speed: 4.0ms preprocess, 959.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 58 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 799.1ms\n",
      "Speed: 2.8ms preprocess, 799.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 59 / 341\n",
      "0: 384x640 26 persons, 1 backpack, 1 tennis racket, 825.2ms\n",
      "Speed: 2.6ms preprocess, 825.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 60 / 341\n",
      "0: 384x640 28 persons, 1 backpack, 1 handbag, 1 tennis racket, 777.6ms\n",
      "Speed: 4.0ms preprocess, 777.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 61 / 341\n",
      "0: 384x640 29 persons, 1 backpack, 1 tennis racket, 787.7ms\n",
      "Speed: 3.2ms preprocess, 787.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 62 / 341\n",
      "0: 384x640 30 persons, 1 handbag, 1 tennis racket, 779.2ms\n",
      "Speed: 2.5ms preprocess, 779.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 63 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 772.6ms\n",
      "Speed: 2.6ms preprocess, 772.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 64 / 341\n",
      "0: 384x640 26 persons, 790.4ms\n",
      "Speed: 3.0ms preprocess, 790.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 65 / 341\n",
      "0: 384x640 25 persons, 785.8ms\n",
      "Speed: 3.3ms preprocess, 785.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 66 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 783.9ms\n",
      "Speed: 2.6ms preprocess, 783.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 67 / 341\n",
      "0: 384x640 28 persons, 2 handbags, 774.8ms\n",
      "Speed: 3.8ms preprocess, 774.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 68 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 787.8ms\n",
      "Speed: 2.5ms preprocess, 787.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 69 / 341\n",
      "0: 384x640 25 persons, 2 handbags, 1066.2ms\n",
      "Speed: 2.5ms preprocess, 1066.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 70 / 341\n",
      "0: 384x640 25 persons, 2 handbags, 1274.8ms\n",
      "Speed: 3.1ms preprocess, 1274.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 71 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1270.3ms\n",
      "Speed: 3.0ms preprocess, 1270.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 72 / 341\n",
      "0: 384x640 32 persons, 1 handbag, 1176.9ms\n",
      "Speed: 2.5ms preprocess, 1176.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 73 / 341\n",
      "0: 384x640 30 persons, 2 handbags, 1 tennis racket, 782.4ms\n",
      "Speed: 3.6ms preprocess, 782.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 74 / 341\n",
      "0: 384x640 28 persons, 1 handbag, 1 tennis racket, 789.8ms\n",
      "Speed: 4.4ms preprocess, 789.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 75 / 341\n",
      "0: 384x640 28 persons, 1 backpack, 1 handbag, 1 tennis racket, 760.3ms\n",
      "Speed: 3.6ms preprocess, 760.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 76 / 341\n",
      "0: 384x640 29 persons, 1 handbag, 1 tennis racket, 786.8ms\n",
      "Speed: 2.6ms preprocess, 786.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 77 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 1 tennis racket, 779.4ms\n",
      "Speed: 3.4ms preprocess, 779.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 78 / 341\n",
      "0: 384x640 29 persons, 1 backpack, 2 handbags, 1 tennis racket, 788.4ms\n",
      "Speed: 2.9ms preprocess, 788.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 79 / 341\n",
      "0: 384x640 29 persons, 2 handbags, 1 tennis racket, 807.0ms\n",
      "Speed: 2.1ms preprocess, 807.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 80 / 341\n",
      "0: 384x640 28 persons, 2 handbags, 1 tennis racket, 778.1ms\n",
      "Speed: 3.9ms preprocess, 778.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 81 / 341\n",
      "0: 384x640 29 persons, 1 handbag, 1 tennis racket, 786.0ms\n",
      "Speed: 6.0ms preprocess, 786.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 82 / 341\n",
      "0: 384x640 29 persons, 1 handbag, 2 tennis rackets, 1058.6ms\n",
      "Speed: 2.1ms preprocess, 1058.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 83 / 341\n",
      "0: 384x640 29 persons, 1 handbag, 2 tennis rackets, 780.9ms\n",
      "Speed: 3.9ms preprocess, 780.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 84 / 341\n",
      "0: 384x640 30 persons, 1 backpack, 2 handbags, 2 tennis rackets, 1045.5ms\n",
      "Speed: 2.7ms preprocess, 1045.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 85 / 341\n",
      "0: 384x640 29 persons, 1 backpack, 2 handbags, 2 tennis rackets, 1235.0ms\n",
      "Speed: 2.9ms preprocess, 1235.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 86 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 2 tennis rackets, 1262.4ms\n",
      "Speed: 2.9ms preprocess, 1262.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 87 / 341\n",
      "0: 384x640 30 persons, 2 handbags, 2 tennis rackets, 1207.2ms\n",
      "Speed: 3.3ms preprocess, 1207.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 88 / 341\n",
      "0: 384x640 30 persons, 1 handbag, 2 tennis rackets, 783.5ms\n",
      "Speed: 2.6ms preprocess, 783.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 89 / 341\n",
      "0: 384x640 29 persons, 1 handbag, 2 tennis rackets, 778.1ms\n",
      "Speed: 2.5ms preprocess, 778.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 90 / 341\n",
      "0: 384x640 30 persons, 1 handbag, 2 tennis rackets, 785.2ms\n",
      "Speed: 4.0ms preprocess, 785.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 91 / 341\n",
      "0: 384x640 28 persons, 1 handbag, 1 tennis racket, 783.9ms\n",
      "Speed: 4.8ms preprocess, 783.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 92 / 341\n",
      "0: 384x640 29 persons, 1 handbag, 1 tennis racket, 795.0ms\n",
      "Speed: 2.8ms preprocess, 795.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 93 / 341\n",
      "0: 384x640 28 persons, 2 handbags, 1 tennis racket, 788.8ms\n",
      "Speed: 2.6ms preprocess, 788.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 94 / 341\n",
      "0: 384x640 28 persons, 1 backpack, 3 handbags, 1 tennis racket, 777.8ms\n",
      "Speed: 3.7ms preprocess, 777.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 95 / 341\n",
      "0: 384x640 30 persons, 1 backpack, 3 handbags, 1 tennis racket, 784.7ms\n",
      "Speed: 3.2ms preprocess, 784.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 96 / 341\n",
      "0: 384x640 30 persons, 1 backpack, 2 handbags, 1 tennis racket, 789.3ms\n",
      "Speed: 3.4ms preprocess, 789.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 97 / 341\n",
      "0: 384x640 32 persons, 2 handbags, 1 tennis racket, 776.3ms\n",
      "Speed: 4.3ms preprocess, 776.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 98 / 341\n",
      "0: 384x640 30 persons, 1 backpack, 1 tennis racket, 786.3ms\n",
      "Speed: 4.4ms preprocess, 786.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 99 / 341\n",
      "0: 384x640 31 persons, 1 backpack, 1 handbag, 1 suitcase, 1 tennis racket, 779.3ms\n",
      "Speed: 2.6ms preprocess, 779.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 100 / 341\n",
      "0: 384x640 30 persons, 1 backpack, 2 handbags, 1 tennis racket, 1266.5ms\n",
      "Speed: 2.6ms preprocess, 1266.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 101 / 341\n",
      "0: 384x640 31 persons, 1 backpack, 2 handbags, 1263.8ms\n",
      "Speed: 6.6ms preprocess, 1263.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 102 / 341\n",
      "0: 384x640 28 persons, 1 backpack, 2 handbags, 1 tennis racket, 1288.1ms\n",
      "Speed: 2.5ms preprocess, 1288.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 103 / 341\n",
      "0: 384x640 28 persons, 1 backpack, 2 handbags, 1 tennis racket, 882.2ms\n",
      "Speed: 2.6ms preprocess, 882.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 104 / 341\n",
      "0: 384x640 28 persons, 1 backpack, 3 handbags, 1 tennis racket, 783.8ms\n",
      "Speed: 2.6ms preprocess, 783.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 105 / 341\n",
      "0: 384x640 32 persons, 1 backpack, 1 handbag, 1 tennis racket, 787.1ms\n",
      "Speed: 2.5ms preprocess, 787.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 106 / 341\n",
      "0: 384x640 30 persons, 1 handbag, 1 tennis racket, 761.2ms\n",
      "Speed: 2.8ms preprocess, 761.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 107 / 341\n",
      "0: 384x640 29 persons, 1 handbag, 1 tennis racket, 787.6ms\n",
      "Speed: 2.7ms preprocess, 787.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 108 / 341\n",
      "0: 384x640 30 persons, 1 handbag, 1 tennis racket, 798.1ms\n",
      "Speed: 2.6ms preprocess, 798.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 109 / 341\n",
      "0: 384x640 30 persons, 2 handbags, 1 tennis racket, 797.5ms\n",
      "Speed: 2.7ms preprocess, 797.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 110 / 341\n",
      "0: 384x640 30 persons, 2 handbags, 1 tennis racket, 791.2ms\n",
      "Speed: 3.4ms preprocess, 791.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 111 / 341\n",
      "0: 384x640 31 persons, 1 handbag, 1 tennis racket, 808.8ms\n",
      "Speed: 2.5ms preprocess, 808.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 112 / 341\n",
      "0: 384x640 29 persons, 2 handbags, 1 tennis racket, 783.4ms\n",
      "Speed: 2.5ms preprocess, 783.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 113 / 341\n",
      "0: 384x640 29 persons, 2 handbags, 1 tennis racket, 794.2ms\n",
      "Speed: 4.9ms preprocess, 794.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 114 / 341\n",
      "0: 384x640 30 persons, 2 handbags, 1 tennis racket, 799.7ms\n",
      "Speed: 6.2ms preprocess, 799.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 115 / 341\n",
      "0: 384x640 29 persons, 3 handbags, 1 tennis racket, 1132.6ms\n",
      "Speed: 3.1ms preprocess, 1132.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 116 / 341\n",
      "0: 384x640 30 persons, 2 handbags, 1 tennis racket, 1248.2ms\n",
      "Speed: 2.5ms preprocess, 1248.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 117 / 341\n",
      "0: 384x640 29 persons, 1 handbag, 1 tennis racket, 1736.7ms\n",
      "Speed: 2.5ms preprocess, 1736.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 118 / 341\n",
      "0: 384x640 29 persons, 2 handbags, 1 tennis racket, 1236.0ms\n",
      "Speed: 2.7ms preprocess, 1236.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 119 / 341\n",
      "0: 384x640 30 persons, 1 handbag, 1 tennis racket, 794.8ms\n",
      "Speed: 3.7ms preprocess, 794.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 120 / 341\n",
      "0: 384x640 29 persons, 1 handbag, 1 tennis racket, 789.6ms\n",
      "Speed: 2.1ms preprocess, 789.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 121 / 341\n",
      "0: 384x640 28 persons, 1 handbag, 1 tennis racket, 809.6ms\n",
      "Speed: 3.0ms preprocess, 809.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 122 / 341\n",
      "0: 384x640 29 persons, 2 handbags, 784.2ms\n",
      "Speed: 2.9ms preprocess, 784.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 123 / 341\n",
      "0: 384x640 29 persons, 2 handbags, 791.9ms\n",
      "Speed: 4.6ms preprocess, 791.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 124 / 341\n",
      "0: 384x640 29 persons, 1 handbag, 789.3ms\n",
      "Speed: 2.8ms preprocess, 789.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 125 / 341\n",
      "0: 384x640 28 persons, 2 handbags, 802.7ms\n",
      "Speed: 2.6ms preprocess, 802.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 126 / 341\n",
      "0: 384x640 29 persons, 2 handbags, 788.4ms\n",
      "Speed: 4.1ms preprocess, 788.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 127 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 785.6ms\n",
      "Speed: 3.9ms preprocess, 785.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 128 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1 tennis racket, 804.0ms\n",
      "Speed: 4.6ms preprocess, 804.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 129 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 1 tennis racket, 785.0ms\n",
      "Speed: 3.1ms preprocess, 785.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 130 / 341\n",
      "0: 384x640 25 persons, 3 handbags, 1207.7ms\n",
      "Speed: 3.3ms preprocess, 1207.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 131 / 341\n",
      "0: 384x640 25 persons, 3 handbags, 1246.3ms\n",
      "Speed: 2.6ms preprocess, 1246.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 132 / 341\n",
      "0: 384x640 23 persons, 1 handbag, 1255.5ms\n",
      "Speed: 2.5ms preprocess, 1255.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 133 / 341\n",
      "0: 384x640 23 persons, 1 handbag, 999.4ms\n",
      "Speed: 2.5ms preprocess, 999.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 134 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 807.8ms\n",
      "Speed: 2.6ms preprocess, 807.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 135 / 341\n",
      "0: 384x640 24 persons, 1 backpack, 2 handbags, 777.2ms\n",
      "Speed: 2.5ms preprocess, 777.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 136 / 341\n",
      "0: 384x640 25 persons, 1 backpack, 2 handbags, 809.1ms\n",
      "Speed: 2.5ms preprocess, 809.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 137 / 341\n",
      "0: 384x640 25 persons, 3 handbags, 1 tennis racket, 798.6ms\n",
      "Speed: 2.6ms preprocess, 798.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 138 / 341\n",
      "0: 384x640 23 persons, 2 handbags, 1 tennis racket, 793.4ms\n",
      "Speed: 3.1ms preprocess, 793.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 139 / 341\n",
      "0: 384x640 22 persons, 2 handbags, 787.8ms\n",
      "Speed: 3.9ms preprocess, 787.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 140 / 341\n",
      "0: 384x640 23 persons, 2 handbags, 782.1ms\n",
      "Speed: 3.3ms preprocess, 782.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 141 / 341\n",
      "0: 384x640 24 persons, 2 handbags, 783.4ms\n",
      "Speed: 2.6ms preprocess, 783.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 142 / 341\n",
      "0: 384x640 27 persons, 3 handbags, 805.1ms\n",
      "Speed: 2.6ms preprocess, 805.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 143 / 341\n",
      "0: 384x640 27 persons, 3 handbags, 812.4ms\n",
      "Speed: 2.6ms preprocess, 812.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 144 / 341\n",
      "0: 384x640 26 persons, 3 handbags, 1 tennis racket, 789.9ms\n",
      "Speed: 2.8ms preprocess, 789.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 145 / 341\n",
      "0: 384x640 27 persons, 3 handbags, 1 tennis racket, 1068.6ms\n",
      "Speed: 2.5ms preprocess, 1068.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 146 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1 tennis racket, 1259.9ms\n",
      "Speed: 3.6ms preprocess, 1259.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 147 / 341\n",
      "0: 384x640 28 persons, 2 handbags, 1287.3ms\n",
      "Speed: 3.1ms preprocess, 1287.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 148 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1117.6ms\n",
      "Speed: 2.6ms preprocess, 1117.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 149 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 1 tennis racket, 798.4ms\n",
      "Speed: 3.8ms preprocess, 798.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 150 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 1 tennis racket, 798.3ms\n",
      "Speed: 4.1ms preprocess, 798.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 151 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 1 tennis racket, 798.5ms\n",
      "Speed: 2.7ms preprocess, 798.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 152 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 1 tennis racket, 824.4ms\n",
      "Speed: 3.7ms preprocess, 824.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 153 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 797.2ms\n",
      "Speed: 2.8ms preprocess, 797.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 154 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 806.2ms\n",
      "Speed: 3.0ms preprocess, 806.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 155 / 341\n",
      "0: 384x640 25 persons, 1 handbag, 1 tennis racket, 805.2ms\n",
      "Speed: 3.6ms preprocess, 805.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 156 / 341\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 806.6ms\n",
      "Speed: 3.1ms preprocess, 806.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 157 / 341\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 808.8ms\n",
      "Speed: 3.0ms preprocess, 808.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 158 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 794.2ms\n",
      "Speed: 4.1ms preprocess, 794.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 159 / 341\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 798.8ms\n",
      "Speed: 2.7ms preprocess, 798.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 160 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1 tennis racket, 1049.1ms\n",
      "Speed: 2.6ms preprocess, 1049.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 161 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 1283.3ms\n",
      "Speed: 3.2ms preprocess, 1283.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 162 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 1280.8ms\n",
      "Speed: 2.7ms preprocess, 1280.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 163 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1 tennis racket, 1133.4ms\n",
      "Speed: 2.7ms preprocess, 1133.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 164 / 341\n",
      "0: 384x640 26 persons, 3 handbags, 1 tennis racket, 804.6ms\n",
      "Speed: 2.7ms preprocess, 804.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 165 / 341\n",
      "0: 384x640 26 persons, 4 handbags, 1 tennis racket, 807.6ms\n",
      "Speed: 2.5ms preprocess, 807.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 166 / 341\n",
      "0: 384x640 26 persons, 3 handbags, 1 tennis racket, 798.9ms\n",
      "Speed: 3.8ms preprocess, 798.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 167 / 341\n",
      "0: 384x640 26 persons, 4 handbags, 1 tennis racket, 802.5ms\n",
      "Speed: 3.2ms preprocess, 802.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 168 / 341\n",
      "0: 384x640 26 persons, 4 handbags, 808.3ms\n",
      "Speed: 2.5ms preprocess, 808.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 169 / 341\n",
      "0: 384x640 26 persons, 3 handbags, 799.8ms\n",
      "Speed: 3.6ms preprocess, 799.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 170 / 341\n",
      "0: 384x640 27 persons, 3 handbags, 1 tennis racket, 811.2ms\n",
      "Speed: 2.5ms preprocess, 811.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 171 / 341\n",
      "0: 384x640 27 persons, 3 handbags, 1 tennis racket, 841.1ms\n",
      "Speed: 3.5ms preprocess, 841.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 172 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 818.8ms\n",
      "Speed: 2.6ms preprocess, 818.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 173 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1 suitcase, 1 tennis racket, 818.6ms\n",
      "Speed: 5.5ms preprocess, 818.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 174 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 812.9ms\n",
      "Speed: 3.2ms preprocess, 812.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 175 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1148.2ms\n",
      "Speed: 3.1ms preprocess, 1148.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 176 / 341\n",
      "0: 384x640 25 persons, 1 handbag, 1279.5ms\n",
      "Speed: 2.9ms preprocess, 1279.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 177 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 1313.7ms\n",
      "Speed: 2.6ms preprocess, 1313.7ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 178 / 341\n",
      "0: 384x640 25 persons, 1 handbag, 1121.4ms\n",
      "Speed: 2.6ms preprocess, 1121.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 179 / 341\n",
      "0: 384x640 25 persons, 1 handbag, 808.0ms\n",
      "Speed: 2.7ms preprocess, 808.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 180 / 341\n",
      "0: 384x640 25 persons, 2 handbags, 800.3ms\n",
      "Speed: 6.5ms preprocess, 800.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 181 / 341\n",
      "0: 384x640 25 persons, 3 handbags, 1427.4ms\n",
      "Speed: 2.6ms preprocess, 1427.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 182 / 341\n",
      "0: 384x640 25 persons, 3 handbags, 814.7ms\n",
      "Speed: 3.9ms preprocess, 814.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 183 / 341\n",
      "0: 384x640 25 persons, 5 handbags, 809.0ms\n",
      "Speed: 3.1ms preprocess, 809.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 184 / 341\n",
      "0: 384x640 26 persons, 4 handbags, 803.7ms\n",
      "Speed: 3.1ms preprocess, 803.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 185 / 341\n",
      "0: 384x640 26 persons, 4 handbags, 809.5ms\n",
      "Speed: 3.5ms preprocess, 809.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 186 / 341\n",
      "0: 384x640 26 persons, 3 handbags, 819.0ms\n",
      "Speed: 2.6ms preprocess, 819.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 187 / 341\n",
      "0: 384x640 26 persons, 4 handbags, 806.8ms\n",
      "Speed: 3.2ms preprocess, 806.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 188 / 341\n",
      "0: 384x640 26 persons, 4 handbags, 809.1ms\n",
      "Speed: 3.9ms preprocess, 809.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 189 / 341\n",
      "0: 384x640 25 persons, 3 handbags, 980.0ms\n",
      "Speed: 2.6ms preprocess, 980.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 190 / 341\n",
      "0: 384x640 26 persons, 3 handbags, 1258.5ms\n",
      "Speed: 2.6ms preprocess, 1258.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 191 / 341\n",
      "0: 384x640 25 persons, 4 handbags, 1258.6ms\n",
      "Speed: 2.7ms preprocess, 1258.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 192 / 341\n",
      "0: 384x640 25 persons, 1 backpack, 3 handbags, 1260.3ms\n",
      "Speed: 6.0ms preprocess, 1260.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 193 / 341\n",
      "0: 384x640 27 persons, 3 handbags, 804.0ms\n",
      "Speed: 3.3ms preprocess, 804.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 194 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 807.6ms\n",
      "Speed: 8.0ms preprocess, 807.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 195 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 805.5ms\n",
      "Speed: 4.0ms preprocess, 805.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 196 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 810.4ms\n",
      "Speed: 2.6ms preprocess, 810.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 197 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 812.7ms\n",
      "Speed: 2.7ms preprocess, 812.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 198 / 341\n",
      "0: 384x640 28 persons, 2 handbags, 812.2ms\n",
      "Speed: 3.0ms preprocess, 812.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 199 / 341\n",
      "0: 384x640 29 persons, 2 handbags, 810.3ms\n",
      "Speed: 2.6ms preprocess, 810.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 200 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1 tennis racket, 807.8ms\n",
      "Speed: 3.5ms preprocess, 807.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 201 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1 tennis racket, 797.8ms\n",
      "Speed: 11.6ms preprocess, 797.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 202 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1 tennis racket, 805.1ms\n",
      "Speed: 3.1ms preprocess, 805.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 203 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1 tennis racket, 788.7ms\n",
      "Speed: 2.6ms preprocess, 788.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 204 / 341\n",
      "0: 384x640 27 persons, 3 handbags, 1 tennis racket, 965.4ms\n",
      "Speed: 2.7ms preprocess, 965.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 205 / 341\n",
      "0: 384x640 28 persons, 2 handbags, 1 tennis racket, 1251.1ms\n",
      "Speed: 2.5ms preprocess, 1251.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 206 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 1 tennis racket, 1266.4ms\n",
      "Speed: 2.6ms preprocess, 1266.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 207 / 341\n",
      "0: 384x640 24 persons, 1 handbag, 1 tennis racket, 1239.5ms\n",
      "Speed: 3.6ms preprocess, 1239.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 208 / 341\n",
      "0: 384x640 25 persons, 3 handbags, 1 tennis racket, 790.1ms\n",
      "Speed: 2.5ms preprocess, 790.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 209 / 341\n",
      "0: 384x640 26 persons, 4 handbags, 1 tennis racket, 785.7ms\n",
      "Speed: 2.5ms preprocess, 785.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 210 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 794.9ms\n",
      "Speed: 3.2ms preprocess, 794.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 211 / 341\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 782.0ms\n",
      "Speed: 2.5ms preprocess, 782.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 212 / 341\n",
      "0: 384x640 24 persons, 2 handbags, 1 tennis racket, 791.4ms\n",
      "Speed: 5.7ms preprocess, 791.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 213 / 341\n",
      "0: 384x640 24 persons, 3 handbags, 804.1ms\n",
      "Speed: 2.6ms preprocess, 804.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 214 / 341\n",
      "0: 384x640 26 persons, 1 backpack, 3 handbags, 793.1ms\n",
      "Speed: 3.8ms preprocess, 793.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 215 / 341\n",
      "0: 384x640 26 persons, 1 backpack, 2 handbags, 786.2ms\n",
      "Speed: 3.4ms preprocess, 786.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 216 / 341\n",
      "0: 384x640 26 persons, 3 handbags, 784.9ms\n",
      "Speed: 2.6ms preprocess, 784.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 217 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 800.3ms\n",
      "Speed: 4.9ms preprocess, 800.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 218 / 341\n",
      "0: 384x640 27 persons, 3 handbags, 798.2ms\n",
      "Speed: 2.7ms preprocess, 798.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 219 / 341\n",
      "0: 384x640 27 persons, 1 backpack, 2 handbags, 838.7ms\n",
      "Speed: 4.6ms preprocess, 838.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 220 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1288.7ms\n",
      "Speed: 2.7ms preprocess, 1288.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 221 / 341\n",
      "0: 384x640 27 persons, 1 backpack, 2 handbags, 1250.1ms\n",
      "Speed: 2.5ms preprocess, 1250.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 222 / 341\n",
      "0: 384x640 28 persons, 1 backpack, 2 handbags, 1306.6ms\n",
      "Speed: 3.7ms preprocess, 1306.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 223 / 341\n",
      "0: 384x640 27 persons, 1 backpack, 2 handbags, 834.6ms\n",
      "Speed: 2.5ms preprocess, 834.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 224 / 341\n",
      "0: 384x640 25 persons, 2 handbags, 789.9ms\n",
      "Speed: 2.5ms preprocess, 789.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 225 / 341\n",
      "0: 384x640 26 persons, 1 backpack, 2 handbags, 806.9ms\n",
      "Speed: 2.5ms preprocess, 806.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 226 / 341\n",
      "0: 384x640 26 persons, 3 handbags, 795.6ms\n",
      "Speed: 2.5ms preprocess, 795.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 227 / 341\n",
      "0: 384x640 25 persons, 2 handbags, 788.1ms\n",
      "Speed: 2.6ms preprocess, 788.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 228 / 341\n",
      "0: 384x640 26 persons, 1 backpack, 4 handbags, 781.8ms\n",
      "Speed: 4.2ms preprocess, 781.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 229 / 341\n",
      "0: 384x640 25 persons, 1 backpack, 3 handbags, 789.1ms\n",
      "Speed: 2.5ms preprocess, 789.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 230 / 341\n",
      "0: 384x640 25 persons, 1 backpack, 3 handbags, 964.6ms\n",
      "Speed: 2.8ms preprocess, 964.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 231 / 341\n",
      "0: 384x640 24 persons, 3 handbags, 799.9ms\n",
      "Speed: 2.5ms preprocess, 799.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 232 / 341\n",
      "0: 384x640 24 persons, 1 backpack, 3 handbags, 798.8ms\n",
      "Speed: 3.0ms preprocess, 798.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 233 / 341\n",
      "0: 384x640 25 persons, 1 backpack, 5 handbags, 797.3ms\n",
      "Speed: 2.5ms preprocess, 797.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 234 / 341\n",
      "0: 384x640 25 persons, 3 handbags, 776.9ms\n",
      "Speed: 2.6ms preprocess, 776.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 235 / 341\n",
      "0: 384x640 25 persons, 4 handbags, 1259.3ms\n",
      "Speed: 2.6ms preprocess, 1259.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 236 / 341\n",
      "0: 384x640 24 persons, 1 backpack, 4 handbags, 1253.9ms\n",
      "Speed: 2.5ms preprocess, 1253.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 237 / 341\n",
      "0: 384x640 25 persons, 1 backpack, 3 handbags, 1316.1ms\n",
      "Speed: 2.5ms preprocess, 1316.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 238 / 341\n",
      "0: 384x640 26 persons, 1 backpack, 2 handbags, 884.9ms\n",
      "Speed: 3.4ms preprocess, 884.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 239 / 341\n",
      "0: 384x640 27 persons, 1 backpack, 2 handbags, 792.1ms\n",
      "Speed: 2.5ms preprocess, 792.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 240 / 341\n",
      "0: 384x640 25 persons, 1 backpack, 2 handbags, 1 tennis racket, 800.4ms\n",
      "Speed: 2.7ms preprocess, 800.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 241 / 341\n",
      "0: 384x640 26 persons, 1 backpack, 2 handbags, 1 tennis racket, 791.7ms\n",
      "Speed: 3.5ms preprocess, 791.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 242 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 785.4ms\n",
      "Speed: 2.5ms preprocess, 785.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 243 / 341\n",
      "0: 384x640 25 persons, 1 backpack, 2 handbags, 793.4ms\n",
      "Speed: 2.8ms preprocess, 793.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 244 / 341\n",
      "0: 384x640 24 persons, 2 handbags, 797.1ms\n",
      "Speed: 2.9ms preprocess, 797.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 245 / 341\n",
      "0: 384x640 24 persons, 1 backpack, 3 handbags, 792.3ms\n",
      "Speed: 2.6ms preprocess, 792.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 246 / 341\n",
      "0: 384x640 23 persons, 2 handbags, 786.6ms\n",
      "Speed: 3.4ms preprocess, 786.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 247 / 341\n",
      "0: 384x640 24 persons, 2 handbags, 773.9ms\n",
      "Speed: 2.6ms preprocess, 773.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 248 / 341\n",
      "0: 384x640 24 persons, 2 handbags, 797.2ms\n",
      "Speed: 2.6ms preprocess, 797.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 249 / 341\n",
      "0: 384x640 26 persons, 1 backpack, 1 suitcase, 791.7ms\n",
      "Speed: 3.8ms preprocess, 791.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 250 / 341\n",
      "0: 384x640 25 persons, 2 handbags, 1 suitcase, 1127.1ms\n",
      "Speed: 2.5ms preprocess, 1127.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 251 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 1266.3ms\n",
      "Speed: 3.2ms preprocess, 1266.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 252 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 1 suitcase, 1 tennis racket, 1266.2ms\n",
      "Speed: 6.6ms preprocess, 1266.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 253 / 341\n",
      "0: 384x640 25 persons, 3 handbags, 1043.3ms\n",
      "Speed: 2.5ms preprocess, 1043.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 254 / 341\n",
      "0: 384x640 28 persons, 3 handbags, 1 tennis racket, 780.6ms\n",
      "Speed: 3.7ms preprocess, 780.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 255 / 341\n",
      "0: 384x640 27 persons, 4 handbags, 1 tennis racket, 790.5ms\n",
      "Speed: 3.1ms preprocess, 790.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 256 / 341\n",
      "0: 384x640 24 persons, 2 handbags, 1 tennis racket, 789.2ms\n",
      "Speed: 2.6ms preprocess, 789.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 257 / 341\n",
      "0: 384x640 25 persons, 1 backpack, 2 handbags, 1 tennis racket, 800.1ms\n",
      "Speed: 3.1ms preprocess, 800.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 258 / 341\n",
      "0: 384x640 24 persons, 3 handbags, 1 suitcase, 1 tennis racket, 795.1ms\n",
      "Speed: 3.2ms preprocess, 795.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 259 / 341\n",
      "0: 384x640 24 persons, 4 handbags, 1 tennis racket, 787.8ms\n",
      "Speed: 3.4ms preprocess, 787.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 260 / 341\n",
      "0: 384x640 25 persons, 1 handbag, 1 tennis racket, 789.5ms\n",
      "Speed: 2.6ms preprocess, 789.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 261 / 341\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 801.3ms\n",
      "Speed: 2.7ms preprocess, 801.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 262 / 341\n",
      "0: 384x640 26 persons, 3 handbags, 1 tennis racket, 802.3ms\n",
      "Speed: 2.6ms preprocess, 802.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 263 / 341\n",
      "0: 384x640 25 persons, 2 handbags, 1 tennis racket, 817.0ms\n",
      "Speed: 2.7ms preprocess, 817.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 264 / 341\n",
      "0: 384x640 24 persons, 1 handbag, 799.5ms\n",
      "Speed: 4.0ms preprocess, 799.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 265 / 341\n",
      "0: 384x640 25 persons, 1 backpack, 1 handbag, 1015.3ms\n",
      "Speed: 2.5ms preprocess, 1015.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 266 / 341\n",
      "0: 384x640 25 persons, 1 backpack, 2 handbags, 1243.3ms\n",
      "Speed: 2.5ms preprocess, 1243.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 267 / 341\n",
      "0: 384x640 26 persons, 1 backpack, 2 handbags, 1271.3ms\n",
      "Speed: 2.6ms preprocess, 1271.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 268 / 341\n",
      "0: 384x640 26 persons, 1 backpack, 2 handbags, 1166.6ms\n",
      "Speed: 3.6ms preprocess, 1166.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 269 / 341\n",
      "0: 384x640 26 persons, 1 backpack, 2 handbags, 819.9ms\n",
      "Speed: 2.5ms preprocess, 819.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 270 / 341\n",
      "0: 384x640 25 persons, 1 backpack, 3 handbags, 786.0ms\n",
      "Speed: 3.8ms preprocess, 786.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 271 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 802.5ms\n",
      "Speed: 2.5ms preprocess, 802.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 272 / 341\n",
      "0: 384x640 25 persons, 2 handbags, 782.6ms\n",
      "Speed: 3.0ms preprocess, 782.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 273 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 784.6ms\n",
      "Speed: 2.5ms preprocess, 784.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 274 / 341\n",
      "0: 384x640 27 persons, 3 handbags, 1 tennis racket, 784.0ms\n",
      "Speed: 2.5ms preprocess, 784.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 275 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 792.0ms\n",
      "Speed: 2.4ms preprocess, 792.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 276 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1 tennis racket, 795.5ms\n",
      "Speed: 2.6ms preprocess, 795.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 277 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 776.5ms\n",
      "Speed: 3.1ms preprocess, 776.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 278 / 341\n",
      "0: 384x640 27 persons, 3 handbags, 1 suitcase, 1 tennis racket, 800.2ms\n",
      "Speed: 2.5ms preprocess, 800.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 279 / 341\n",
      "0: 384x640 28 persons, 4 handbags, 1 suitcase, 1 tennis racket, 798.6ms\n",
      "Speed: 3.2ms preprocess, 798.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 280 / 341\n",
      "0: 384x640 27 persons, 3 handbags, 1 suitcase, 1 tennis racket, 906.0ms\n",
      "Speed: 2.6ms preprocess, 906.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 281 / 341\n",
      "0: 384x640 28 persons, 2 handbags, 1 tennis racket, 1267.4ms\n",
      "Speed: 2.5ms preprocess, 1267.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 282 / 341\n",
      "0: 384x640 26 persons, 3 handbags, 1 tennis racket, 1242.1ms\n",
      "Speed: 2.8ms preprocess, 1242.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 283 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1 tennis racket, 1285.2ms\n",
      "Speed: 2.5ms preprocess, 1285.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 284 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 1 suitcase, 1 tennis racket, 788.2ms\n",
      "Speed: 3.8ms preprocess, 788.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 285 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 1 suitcase, 1 tennis racket, 789.2ms\n",
      "Speed: 4.2ms preprocess, 789.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 286 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 1 tennis racket, 789.5ms\n",
      "Speed: 3.4ms preprocess, 789.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 287 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 1 tennis racket, 801.5ms\n",
      "Speed: 2.6ms preprocess, 801.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 288 / 341\n",
      "0: 384x640 28 persons, 1 handbag, 1 tennis racket, 799.8ms\n",
      "Speed: 3.9ms preprocess, 799.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 289 / 341\n",
      "0: 384x640 28 persons, 1 handbag, 1 tennis racket, 781.4ms\n",
      "Speed: 2.7ms preprocess, 781.4ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 290 / 341\n",
      "0: 384x640 28 persons, 1 handbag, 1 tennis racket, 789.7ms\n",
      "Speed: 2.6ms preprocess, 789.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 291 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 1 tennis racket, 788.6ms\n",
      "Speed: 3.1ms preprocess, 788.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 292 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 1 tennis racket, 791.7ms\n",
      "Speed: 2.6ms preprocess, 791.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 293 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1 tennis racket, 798.6ms\n",
      "Speed: 3.6ms preprocess, 798.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 294 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 1 tennis racket, 788.4ms\n",
      "Speed: 2.6ms preprocess, 788.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 295 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 1 tennis racket, 806.1ms\n",
      "Speed: 2.6ms preprocess, 806.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 296 / 341\n",
      "0: 384x640 26 persons, 1 handbag, 1 tennis racket, 1253.0ms\n",
      "Speed: 2.6ms preprocess, 1253.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 297 / 341\n",
      "0: 384x640 28 persons, 1 backpack, 1 handbag, 1 tennis racket, 1267.1ms\n",
      "Speed: 2.5ms preprocess, 1267.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 298 / 341\n",
      "0: 384x640 30 persons, 3 handbags, 1296.7ms\n",
      "Speed: 2.5ms preprocess, 1296.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 299 / 341\n",
      "0: 384x640 29 persons, 3 handbags, 899.5ms\n",
      "Speed: 2.6ms preprocess, 899.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 300 / 341\n",
      "0: 384x640 25 persons, 1 handbag, 799.1ms\n",
      "Speed: 2.6ms preprocess, 799.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 301 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 830.2ms\n",
      "Speed: 2.5ms preprocess, 830.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 302 / 341\n",
      "0: 384x640 27 persons, 1 handbag, 778.1ms\n",
      "Speed: 2.6ms preprocess, 778.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 303 / 341\n",
      "0: 384x640 30 persons, 1 handbag, 794.9ms\n",
      "Speed: 3.0ms preprocess, 794.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 304 / 341\n",
      "0: 384x640 28 persons, 2 handbags, 793.4ms\n",
      "Speed: 6.1ms preprocess, 793.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 305 / 341\n",
      "0: 384x640 28 persons, 1 handbag, 798.4ms\n",
      "Speed: 3.0ms preprocess, 798.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 306 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 792.9ms\n",
      "Speed: 3.2ms preprocess, 792.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 307 / 341\n",
      "0: 384x640 29 persons, 1 handbag, 781.4ms\n",
      "Speed: 2.6ms preprocess, 781.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 308 / 341\n",
      "0: 384x640 29 persons, 3 handbags, 790.6ms\n",
      "Speed: 6.0ms preprocess, 790.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 309 / 341\n",
      "0: 384x640 30 persons, 1 backpack, 1 handbag, 795.7ms\n",
      "Speed: 2.6ms preprocess, 795.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 310 / 341\n",
      "0: 384x640 30 persons, 1 handbag, 797.5ms\n",
      "Speed: 2.7ms preprocess, 797.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 311 / 341\n",
      "0: 384x640 30 persons, 2 handbags, 1160.0ms\n",
      "Speed: 3.6ms preprocess, 1160.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 312 / 341\n",
      "0: 384x640 30 persons, 3 handbags, 1262.0ms\n",
      "Speed: 2.5ms preprocess, 1262.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 313 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 1265.1ms\n",
      "Speed: 4.7ms preprocess, 1265.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 314 / 341\n",
      "0: 384x640 26 persons, 3 handbags, 999.2ms\n",
      "Speed: 2.5ms preprocess, 999.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 315 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 796.5ms\n",
      "Speed: 3.1ms preprocess, 796.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 316 / 341\n",
      "0: 384x640 29 persons, 2 handbags, 804.4ms\n",
      "Speed: 4.1ms preprocess, 804.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 317 / 341\n",
      "0: 384x640 29 persons, 3 handbags, 794.4ms\n",
      "Speed: 3.2ms preprocess, 794.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 318 / 341\n",
      "0: 384x640 28 persons, 3 handbags, 794.4ms\n",
      "Speed: 2.5ms preprocess, 794.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 319 / 341\n",
      "0: 384x640 28 persons, 2 handbags, 785.5ms\n",
      "Speed: 2.5ms preprocess, 785.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 320 / 341\n",
      "0: 384x640 28 persons, 1 bicycle, 3 handbags, 779.5ms\n",
      "Speed: 4.0ms preprocess, 779.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 321 / 341\n",
      "0: 384x640 29 persons, 1 handbag, 788.6ms\n",
      "Speed: 3.7ms preprocess, 788.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 322 / 341\n",
      "0: 384x640 29 persons, 1 handbag, 799.1ms\n",
      "Speed: 2.5ms preprocess, 799.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 323 / 341\n",
      "0: 384x640 31 persons, 2 handbags, 792.1ms\n",
      "Speed: 2.6ms preprocess, 792.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 324 / 341\n",
      "0: 384x640 31 persons, 2 handbags, 790.2ms\n",
      "Speed: 2.5ms preprocess, 790.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 325 / 341\n",
      "0: 384x640 30 persons, 1 backpack, 2 handbags, 785.1ms\n",
      "Speed: 2.7ms preprocess, 785.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 326 / 341\n",
      "0: 384x640 28 persons, 2 handbags, 1067.3ms\n",
      "Speed: 3.1ms preprocess, 1067.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 327 / 341\n",
      "0: 384x640 30 persons, 1 handbag, 1279.2ms\n",
      "Speed: 3.1ms preprocess, 1279.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 328 / 341\n",
      "0: 384x640 28 persons, 2 handbags, 1271.5ms\n",
      "Speed: 2.5ms preprocess, 1271.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 329 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 1110.6ms\n",
      "Speed: 2.5ms preprocess, 1110.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 330 / 341\n",
      "0: 384x640 28 persons, 2 handbags, 793.9ms\n",
      "Speed: 2.5ms preprocess, 793.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 331 / 341\n",
      "0: 384x640 29 persons, 2 handbags, 799.9ms\n",
      "Speed: 2.5ms preprocess, 799.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 332 / 341\n",
      "0: 384x640 28 persons, 1 handbag, 781.6ms\n",
      "Speed: 2.5ms preprocess, 781.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 333 / 341\n",
      "0: 384x640 26 persons, 3 handbags, 800.8ms\n",
      "Speed: 2.7ms preprocess, 800.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 334 / 341\n",
      "0: 384x640 27 persons, 2 handbags, 812.2ms\n",
      "Speed: 3.0ms preprocess, 812.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 335 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 789.8ms\n",
      "Speed: 2.6ms preprocess, 789.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 336 / 341\n",
      "0: 384x640 26 persons, 3 handbags, 805.1ms\n",
      "Speed: 4.6ms preprocess, 805.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 337 / 341\n",
      "0: 384x640 26 persons, 3 handbags, 789.9ms\n",
      "Speed: 3.4ms preprocess, 789.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 338 / 341\n",
      "0: 384x640 27 persons, 3 handbags, 787.7ms\n",
      "Speed: 2.5ms preprocess, 787.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 339 / 341\n",
      "0: 384x640 25 persons, 2 handbags, 790.8ms\n",
      "Speed: 2.5ms preprocess, 790.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "count: 340 / 341\n",
      "0: 384x640 26 persons, 2 handbags, 794.5ms\n",
      "Speed: 2.4ms preprocess, 794.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# @title 侵入検知\n",
    "\n",
    "for _ in tqdm(range(video_src.frame_count)):\n",
    "    # 1. フレームの読み込み\n",
    "    drifting = video_src.read()\n",
    "    if drifting is None:  # 動画終了のチェック\n",
    "        break\n",
    "\n",
    "    # 2. 物体検出の実行\n",
    "    drifting = object_detector.predict(drifting)\n",
    "\n",
    "    # 3. 侵入判定\n",
    "    drifting = roi_watcher.detect(drifting)\n",
    "\n",
    "    # 4. 検出結果の描画\n",
    "    drifting = detection_renderer.draw(drifting)\n",
    "\n",
    "    # 5. 結果の書き出し\n",
    "    drifting = video_sink.write(drifting)\n",
    "\n",
    "# 6. 終了処理\n",
    "video_sink.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jui_OfEqCppu"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f6a2fd9ca5540bcb5a06b7e680dfcb9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f77d6edfbfd4e1ab721ad558836df19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90eae9d1ca6545e6aac33d130a182e27",
      "placeholder": "​",
      "style": "IPY_MODEL_95afed690d524570b375da4e936ebe01",
      "value": "100%"
     }
    },
    "4032d42ddf174a04aac2cd24f1c403df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2f77d6edfbfd4e1ab721ad558836df19",
       "IPY_MODEL_73cb7ba6be7a447abee99d43e990481b",
       "IPY_MODEL_9743db91db0d4c289713ba3dadbd4bda"
      ],
      "layout": "IPY_MODEL_c73caa96bb044ca4bf9afdf4c854e219"
     }
    },
    "47208a59c7da41779232f71bfa98683f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "73cb7ba6be7a447abee99d43e990481b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0f6a2fd9ca5540bcb5a06b7e680dfcb9",
      "max": 341,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_47208a59c7da41779232f71bfa98683f",
      "value": 341
     }
    },
    "8083397b249d4586917477d8a4ece502": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b5f7199af4b40fe893a1465693929c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90eae9d1ca6545e6aac33d130a182e27": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95afed690d524570b375da4e936ebe01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9743db91db0d4c289713ba3dadbd4bda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8083397b249d4586917477d8a4ece502",
      "placeholder": "​",
      "style": "IPY_MODEL_8b5f7199af4b40fe893a1465693929c9",
      "value": " 341/341 [05:25&lt;00:00,  1.20it/s]"
     }
    },
    "c73caa96bb044ca4bf9afdf4c854e219": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
